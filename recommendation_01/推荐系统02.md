黑马推荐课程体系



![img](D:/002--------------/create6@126.com/99b79e964cc94ad6a231c185d0297687/clipboard.png)

### 推荐系统简介



![1565924915681](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1565924915681.png)

![1565924964620](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1565924964620.png)





#### Recommender Systems

*Libraries for building recommender systems.*



##### annoy

- [annoy](https://github.com/spotify/annoy) - Approximate Nearest Neighbors in C++/Python optimized for memory usage.



##### fastFM

- [fastFM](https://github.com/ibayer/fastFM) - A library for Factorization Machines.



##### implicit

- [implicit](https://github.com/benfred/implicit) - A fast Python implementation of  **collaborative filtering** for implicit datasets.

  - 隐式数据集的快速Python协同过滤

    该项目为隐式反馈数据集提供了几种不同流行**推荐算法**的快速Python实现：

    - 如[隐式反馈数据集的协同过滤](http://yifanhu.net/PUB/cf.pdf)和[隐式反馈协同过滤的共轭梯度法的应用中所述的](https://pdfs.semanticscholar.org/bfdf/7af6cf7fd7bb5e6b6db5bbd91be11597eaf0.pdf)交替最小二乘法。
    - [贝叶斯个性化排名](https://arxiv.org/pdf/1205.2618.pdf)
    - [Logistic矩阵分解](https://web.stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf)
    - 项目项最近邻模型使用余弦，TFIDF或BM25作为距离度量

    所有型号都具有多线程训练程序，使用Cython和OpenMP在所有可用CPU核心之间并行匹配模型。此外，ALS和BPR模型都具有自定义CUDA内核 - 可以兼容兼容的GPU。Implicit也可以使用近似的最近邻库（如[Annoy](https://github.com/spotify/annoy)，[NMSLIB](https://github.com/searchivarius/nmslib) 和[Faiss）](https://github.com/facebookresearch/faiss)





##### libffm

- [libffm](https://github.com/guestwalk/libffm) - A library for Field-aware Factorization Machine (FFM).

#Factorization #因数分解



##### LightFM

- [lightfm](https://github.com/lyst/lightfm) - A Python implementation of a number of popular recommendation algorithms.

  - LightFM是针对隐式和显式反馈的许多流行推荐算法的Python实现，包括BPR和WARP排名损失的有效实现。它易于使用，快速（通过多线程模型估计），并产生高质量的结果。

    它还可以将项目和用户元数据合并到传统的矩阵分解算法中。它将每个用户和项目表示为其功能的潜在表示的总和，从而允许推荐推广到新项目（通过项目功能）和新用户（通过用户功能）。

  - ```python
    from lightfm import LightFM
    from lightfm.datasets import fetch_movielens
    from lightfm.evaluation import precision_at_k
    
    # Load the MovieLens 100k dataset. Only five
    # star ratings are treated as positive.
    data = fetch_movielens(min_rating=5.0)
    
    # Instantiate and train the model
    model = LightFM(loss='warp')
    model.fit(data['train'], epochs=30, num_threads=2)
    
    # Evaluate the trained model
    test_precision = precision_at_k(model, data['test'], k=5).mean()
    ```



##### spotlight

- [spotlight](https://github.com/maciejkula/spotlight) - Deep recommender models using PyTorch.

  - Spotlight使用[PyTorch](http://pytorch.org/)构建深度和浅度推荐器模型。通过提供一系列用于损失函数的构建块（各种逐点和成对排序损失），表示（浅分解表示，深度序列模型）以及用于获取（或生成）推荐数据集的实用程序，它旨在成为快速的工具新推荐模型的探索和原型设计。

  - 要在MovieLens数据集上拟合显式反馈模型：

  - ```python
    from spotlight.cross_validation import random_train_test_split
    from spotlight.datasets.movielens import get_movielens_dataset
    from spotlight.evaluation import rmse_score
    from spotlight.factorization.explicit import ExplicitFactorizationModel
    
    dataset = get_movielens_dataset(variant='100K')
    
    train, test = random_train_test_split(dataset)
    
    model = ExplicitFactorizationModel(n_iter=1)
    model.fit(train)
    
    rmse = rmse_score(model, test)
    ```


##### Surprise
- [Surprise](https://github.com/NicolasHug/Surprise) - A scikit for building and analyzing recommender systems.
  - 让用户完美控制他们的实验。为此，我们非常强调 [文档](http://surprise.readthedocs.io/en/stable/index.html)，我们试图通过指出算法的每个细节尽可能清晰和准确。
  - 减轻[数据集处理](http://surprise.readthedocs.io/en/stable/getting_started.html#load-a-custom-dataset)的痛苦。用户可以使用*内置*数据集（[Movielens](http://grouplens.org/datasets/movielens/)， [Jester](http://eigentaste.berkeley.edu/dataset/)）和他们自己的*自定义* 数据集。
  - 提供各种即用型[预测算法](http://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html) ，如[基线算法](http://surprise.readthedocs.io/en/stable/basic_algorithms.html)， [邻域方法](http://surprise.readthedocs.io/en/stable/knn_inspired.html)，基于矩阵因子分解（ [SVD](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD)， [PMF](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#unbiased-note)， [SVD ++](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp)， [NMF](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF)）[等等](http://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html)。此外，内置了各种[相似性度量](http://surprise.readthedocs.io/en/stable/similarities.html)（余弦，MSD，皮尔逊......）。
  - 可以轻松实现[新的算法思路](http://surprise.readthedocs.io/en/stable/building_custom_algo.html)。
  - 提供[评估](http://surprise.readthedocs.io/en/stable/model_selection.html)， [分析](http://nbviewer.jupyter.org/github/NicolasHug/Surprise/tree/master/examples/notebooks/KNNBasic_analysis.ipynb/) 和 [比较](http://nbviewer.jupyter.org/github/NicolasHug/Surprise/blob/master/examples/notebooks/Compare.ipynb) 算法性能的工具。使用强大的CV迭代器（受[scikit-learn](http://scikit-learn.org/)优秀工具启发）以及 [对一组参数的详尽搜索，](http://surprise.readthedocs.io/en/stable/getting_started.html#tune-algorithm-parameters-with-gridsearchcv)可以非常轻松地运行交叉验证程序 。





##### TensorRec

![1566392073808](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566392073808.png)

- [tensorrec](https://github.com/jfkirk/tensorrec) - A Recommendation Engine Framework in TensorFlow.

  - TensorRec是一个Python推荐系统，允许您快速开发推荐算法并使用TensorFlow自定义它们。

    TensorRec允许您自定义推荐系统的表示/嵌入功能和丢失功能，而TensorRec处理数据操作，评分和排名以生成推荐。

    一个TensorRec系统消耗三个部分数据：`user_features`，`item_features`，和`interactions`,它使用此数据来学习制作和排名建议。

    有关TensorRec及其用法的概述，请参阅[wiki](https://github.com/jfkirk/tensorrec/wiki)

    有关更多信息以及此项目的概要，请阅读[此博客文章](https://medium.com/@jameskirk1/tensorrec-a-recommendation-engine-framework-in-tensorflow-d85e4f0874e8)

    有关构建推荐系统的介绍，请参阅[这些幻灯片](https://www.slideshare.net/JamesKirk58/boston-ml-architecting-recommender-systems)

  - ```python
    import numpy as np
    import tensorrec
    
    # Build the model with default parameters
    model = tensorrec.TensorRec()
    
    # Generate some dummy data
    interactions, user_features, item_features = tensorrec.util.generate_dummy_data(
        num_users=100,
        num_items=150,
        interaction_density=.05
    )
    
    # Fit the model for 5 epochs
    model.fit(interactions, user_features, item_features, epochs=5, verbose=True)
    
    # Predict scores and ranks for all users and all items
    predictions = model.predict(user_features=user_features,
                                item_features=item_features)
    predicted_ranks = model.predict_rank(user_features=user_features,
                                         item_features=item_features)
    
    # Calculate and print the recall at 10
    r_at_k = tensorrec.eval.recall_at_k(predicted_ranks, interactions, k=10)
    print(np.mean(r_at_k))
    ```




浏览-搜索-推荐-广告

搜索:主动,需要提供关键词

推荐:被动,不需要提供关键词,根据客户喜好推荐,排序,锦上添花,解决信息过载,用户没有明确需求(eg.商场导购)



eg.推荐电影,推荐汽车,推荐房产,推荐股票,推荐贷款产品



#### 推荐系统 V.S. 搜索引擎

|          | 搜索     | 推荐     |
| -------- | -------- | -------- |
| 行为方式 | 主动     | 被动     |
| 意图     | 明确     | 模糊     |
| 个性化   | 弱       | 强       |
| 流量分布 | 马太效应 | 长尾效应 |
| 目标     | 快速满足 | 持续服务 |
| 评估指标 | 简明     | 复杂     |





##### 推荐系统原理

社会化推荐:朋友咨询,让好友推荐

基于内容的推荐:通过文本找出相似的内容

基于流行度推荐,排行榜

基于协同过滤的推荐:找到兴趣小组,找有相似兴趣的用户或小组,看看他们最近的行为(eg.看的电影),物以类聚,人以群分



##### 作用:

高效连接用户和物品,

提高用户停留时间和用户活跃程序,

有效的帮助产品实现其商业价值(会有业绩考核,转化率指标)

应用场景:头条,淘宝,垂直领域领头企业





#### 推荐系统和Web项目的区别

- 通过信息过滤实现目标提升 V.S. 稳定的信息流通系统

- - web项目: 处理复杂业务逻辑，处理高并发，为用户构建一个稳定的信息流通服务
  - 推荐系统: 追求指标增长, 留存率/阅读时间/GMV (Gross Merchandise Volume电商网站成交金额)/视频网站VV (Video View)

- 确定 V.S. 不确定思维

- - web项目: 对结果有确定预期
  - 推荐系统: 结果是概率问题,**不确定性**

概率思想,统计思想





#### 推荐系统要素

- UI 和 UE(前端界面)
- 数据 (Lambda架构)
- 业务知识
- 算法





1EB = 1024 PB

1PB = 1024 TB

1TB = 1024 GB



#### 大数据Lambda架构

- Lambda架构是由**实时**大数据处理框架Storm的作者Nathan Marz提出的一个实时大数据处理框架。
- Lambda架构的将离线计算和实时计算整合，设计出一个能满足实时大数据系统关键特性的架构，包括有：高容错、低延时和可扩展等。
- 离线计算(计算量大,耗时长,Hadoop,模型训练)



- 离线计算
  - 慢 处理的数据量比较大
  - hadoop
    - hdfs 数据存储
    - mapreduce
  - 模型训练
  - 数据处理
- 实时计算
  - 低延迟
  - 处理的数据量会小
  - 训练好的模型加载 实时的排序
  - 用户的实时特征变化 捕捉 更新数据
- Lambda架构解决的问题
  - 离线计算和实时计算协同工作的问题
  - 高容错、低延时和可扩展 的服务
- 推荐系统当中lambda架构的作用
  - 离线计算训练模型
  - 实时计算提供推荐服务
  - 实时计算还可以实时调整用户的特征和用户感兴趣的商品结果



#### 分层架构

- 批处理层
  - 数据不可变, 可进行任何计算, 可水平扩展
  - 高延迟 几分钟~几小时(计算量和数据量不同)
  - 日志收集： Flume
  - 分布式存储： Hadoop
  - 分布式计算： Hadoop、Spark
  - 视图存储数据库
    - nosql(HBase/Cassandra)
    - Redis/memcache
    - MySQL
- 实时处理层
  - 流式处理, 持续计算
  - 存储和分析某个窗口期内的数据（一段时间的热销排行，实时热搜等）
  - 实时数据收集 flume & kafka
  - 实时数据分析 spark streaming/storm/flink
- 服务层
  - 支持随机读
  - 需要在非常短的时间内返回结果
  - 读取批处理层和实时处理层结果并对其归并





#### Lambda架构图

- 数据收集
  - Flume

- 实时处理
  - Kafka 
  - SparkStreaming
  - Storm
  - Flink
- 批处理(离线)
  - HDFS
  - HBase
  - MySQL

![img](file:///D:/002--------------/create6@126.com/41f0587cc7464d3ebc2ab05951241fd4/clipboard.png)





### 推荐算法架构

#### 1,召回阶段 (海选)  recall

召回决定了最终推荐结果的天花板

##### 常用算法

- 协同过滤 cf  (Collaborative Filtering)
- 基于内容 cb (content based)



#### 2.排序阶段 （精选）ranking

- 召回决定了最终推荐结果的天花板, 排序逼近这个极限, 决定了最终的推荐效果

  CTR预估 (点击率预估 使用LR算法) 估计用户是否会点击某个商品 需要用户的点击数据

  逻辑回归 LogisticRegression ,无法根据特征

  0,1,估计用户点击某一个item概率

  根据概率去排序



#### 3.策略调整

- 过滤掉用户看了多次没反应的
- 商业合作,广告置顶,竞价排名
- 加入特征:时间点,价格区间

![img](D:/002--------------/create6@126.com/10c0866c6026487baa008197b3fff1e6/clipboard.png)



##### SPARK（计算引擎）

Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类Hadoop MapReduce的通用并行框架，Spark，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是——Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。



Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。





#### 推荐模型构建流程

类似机器学习流程

数据处理-->特征工程-->训练算法模型-->评估上线

Data(数据)->Features(特征)->ML Algorithm(选择算法训练模型)->Prediction Output(预测输出)

- 数据清洗/数据处理
  - 数据来源
    - 显性数据
      - Rating 打分
      - Comments 评论/评价
    - 隐形数据
      -  Order history 历史订单
      -  Cart events 加购物车
      -  Page views 页面浏览
      -  Click-thru 点击
      -  Search log 搜索记录
  - 数据量/数据能否满足要求
- 特征工程
  - 从数据中筛选特征
    - 一个给定的商品，可能被拥有类似品味或需求的用户购买
    - 使用用户行为数据描述商品



#### 推荐系统算法



##### 1.基于协同过滤的推荐 CF

![1565925633414](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1565925633414.png)

##### 2.基于内容的推荐 CB

![1565925726385](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1565925726385.png)

##### 3.基于人口统计学的推荐



##### 4.混合推荐

![1565926065774](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1565926065774.png)







### 协同过滤算法 CF

（Collaborative Filtering）

#### 基于近邻的协同过滤Memory-Based 

Memory-Based CF



Memory-Based利用用户行为数据计算相似度,要求数据量大,而Model_Based可以是稀疏矩阵

基于用户的协同过滤

基于物品的协同过滤

特征工程 需要把准备好

计算相似度

 - 杰卡德 01
 - 余弦/皮尔逊 连续的评分数据

根据相似度找到最相似的用户/最相似的物品

协同过滤对用户行为数据量要求比较高,数量量大时效果好



##### 算法思想：

基本的协同过滤推荐算法基于以下假设：

- “跟你喜好相似的人喜欢的东西你也很有可能喜欢” ：基于用户的协同过滤推荐（User-based CF）人以群分
- “跟你喜欢的东西相似的东西你也很有可能喜欢 ”：基于物品的协同过滤推荐（Item-based CF）物以类聚

实现协同过滤推荐有以下几个步骤：

1. 找出最相似的人或物品：TOP-N相似的人或物品

通过计算两两的相似度来进行排序，即可找出TOP-N相似的人或物品

1. 根据相似的人或物品产生推荐结果

利用TOP-N结果生成初始推荐结果，然后过滤掉用户已经有过记录的物品或明确表示不感兴趣的物品

以下是一个简单的示例，数据集相当于一个用户对物品的购买记录表：打勾表示用户对物品的有购买记录

- 关于相似度计算这里先用一个简单的思想：如有两个同学X和Y，X同学爱好[足球、篮球、乒乓球]，Y同学爱好[网球、足球、篮球、羽毛球]，可见他们的共同爱好有2个，那么他们的相似度可以用：2/3 * 2/4 = 1/3 ≈ 0.33 来表示。

![img](D:/002--------------/create6@126.com/aae1613debf74fc3b4816f5c6781ec7b/clipboard.png)

 欧氏距离的值是一个非负数, 最大值正无穷, 通常计算相似度的结果希望是[-1,1]或[0,1]之间,一般可以使用

 如下转化公式:

![img](file:///D:/002--------------/create6@126.com/e932e07c42534ea5a25be0b1eddeadba/clipboard.png)

##### 相似度与相似距离

各个相似度对比

余弦相似度与方向相关于距离无关,应对方案:皮尔逊

余弦相似度与皮尔逊相关系数Pearson不适合计算布尔值(01分布,买与不买)向量之间的相关度

01分布用杰卡德相似度



##### 杰卡德相似性度量

（1）杰卡德相似系数

两个集合A和B交集元素的个数在A、B并集中所占的比例，称为这两个集合的杰卡德系数，用符号 J(A,B) 表示。杰卡德相似系数是衡量两个集合相似度的一种指标（余弦距离也可以用来衡量两个集合的相似度）。

![img](D:/002--------------/create6@126.com/84c9713079854ad3a9f6aff05d73ad93/2ea8d10e3d33.gif)

（2）杰卡德距离(不相似的)

与杰卡德相似系数相反的概念是杰卡德距离（Jaccard Distance），可以用如下公式来表示：

![img](D:/002--------------/create6@126.com/b57df45c8f93481690c3b6311b154460/18ab4d4b0e8e.gif)

**杰卡德距离 +  杰卡德相似系数 = 1**



![img](file:///D:/002--------------/create6@126.com/51d3acaf5d13413caa6c2a6487a87f5e/clipboard.png)





pandas中corr方法可直接用于计算皮尔逊相关系数

计算相似度可以进行小**优化**,(对角)减少计算量,eg.九九乘法表的梯度



##### 皮尔逊相关系数Pearson

- 实际上也是余弦相似度, 不过先对向量做了中心化, 向量a b各自减去向量的均值后, 再计算余弦相似度
- 皮尔逊相似度计算结果在-1,1之间 -1表示负相关, 1表示正相关
- 度量两个变量是不是同增同减
- 皮尔逊相关系数度量的是两个变量的变化趋势是否一致, **不适合计算布尔值向量之间的相关度**



![img](D:/002--------------/create6@126.com/31cc963452b74d6e83c19652274f431c/clipboard.png)



##### 余弦距离和欧氏距离的对比

余弦距离使用两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比欧氏距离，余弦距离更加注重两个向量在方向上的差异。

借助三维坐标系来看下欧氏距离和余弦距离的区别：

![img](D:/002--------------/create6@126.com/9d9af94774984a22b8b2e4b69ff8911e/7f1eaf13d0b.jpeg)

从上图可以看出，欧氏距离衡量的是空间各点的绝对距离，跟各个点所在的位置坐标直接相关；而余弦距离衡量的是空间向量的夹角，更加体现在方向上的差异，而不是位置。如果保持A点位置不变，B点朝原方向远离坐标轴原点，那么这个时候余弦距离 

![img](D:/002--------------/create6@126.com/a29537daab864859ae8d863c8697ae2c/acfcda9d2b6e.gif)

 是保持不变的（因为夹角没有发生变化），而A、B两点的距离显然在发生改变，这就是欧氏距离和余弦距离之间的不同之处。

欧氏距离和余弦距离各自有不同的计算方式和衡量特征，因此它们适用于不同的数据分析模型：

欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异。

余弦距离更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦距离对绝对数值不敏感）。

 



预测用户对物品的评分 （以用户1对电影1评分为例）
$$
评分公式  pred(u,i)=r^ui=∑v∈Usim(u,v)∗rvi∑v∈U|sim(u,v)|
$$




![img](file:///D:/002--------------/create6@126.com/909b6647161a4f958a13365a263b6cc9/clipboard.png)







##### User_CF和Item_CF比较

User_CF适合信息流方向,内容更新极快,例如新闻,小视频,直播

![img](D:/002--------------/create6@126.com/fdc6b896ead6443ebaf24be94cc592d9/clipboard.png)



![img](D:/002--------------/create6@126.com/50b9f831c7fe4f69ab0d796363a226c2/clipboard.png)

##### 协同过滤推荐优缺点

![img](D:/002--------------/create6@126.com/bb040e515d684eb7b27fc36eaaf8764d/clipboard.png)






#### 推荐评估

准确性

- rmse,mae精准率,召回率
- 业务指标是否提升

##### 指标

- 准确度

- 召回率
- 覆盖率
- 多样性

![1565939489116](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1565939489116.png)

![1565938308906](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1565938308906.png)

![1565938403749](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1565938403749.png)

##### 覆盖率与熵



##### 在线评估

灰度发布(低比例测试,慢慢调高比例)

A/B测试(对照组)



随机推荐,热门推荐

指标与业绩平衡





#### 推荐系统的冷启动问题

##### 用户冷启动(新用户)

![img](D:/002--------------/create6@126.com/d45b3e83424744c0ac891db59353caeb/clipboard.png)



方案:

1. 尽可能收集各种用户的信息,给用户打各种标签
2. 通过标签做用户的聚类,通过user_cf推荐
3. 可以考虑使用热门推荐



手机电量状况与心理状态

热门推荐与随机推荐



##### 物品冷启动

![img](D:/002--------------/create6@126.com/3dd118a42c764862a6da742568d63a76/clipboard.png)

利用标签找到相似的物品,相似的物品可能有消费记录

基于内容的推荐





应用方向:

![img](D:/002--------------/create6@126.com/10323469c18b49f5a56229646928fbe6/clipboard.png)

eg.推荐电影,推荐汽车,推荐房产,推荐股票,推荐贷款产品



##### 系统冷启动(新用户 +　新物品）



- 基于内容的推荐 系统早期
- 基于内容的推荐逐渐过渡到协同过滤
- 基于内容的推荐和协同过滤的推荐结果都计算出来 加权求和得到最终推荐结果



### 基于模型的协同过滤Model-Based

Model-Based CF算法

Model_Based可以是稀疏矩阵,Memory-Based利用用户行为数据计算相似度,要求数据量大,



- 基于分类算法、回归算法、聚类算法
- 基于矩阵分解的推荐
- 基于神经网络算法
- 基于图模型算法



- 奇异值分解（SVD）
- 潜在语义分析（LSA）
- 支撑向量机（SVM)



#### 1-基于回归模型的协同过滤推荐

Baseline

梯度下降推导

使用Baseline的算法思想预测评分的步骤如下：

- 计算所有电影的平均评分μμ（即全局平均评分）
- 计算每个用户评分与平均评分μ的偏置值buμ的偏置值bu
- 计算每部电影所接受的评分与平均评分μ的偏置值biμ的偏置值bi
- 预测用户对电影的评分： 
$$
r_u,_i=bui=μ+bu+bi 
$$

偏导数:

在数学中，一个多变量的函数的偏导数，就是它关于其中一个变量的导数而保持其他变量恒定（相对于全导数，在其中所有变量都允许变化）。偏导数在[向量分析](https://baike.baidu.com/item/%E5%90%91%E9%87%8F%E5%88%86%E6%9E%90/10564843)和[微分](https://baike.baidu.com/item/%E5%BE%AE%E5%88%86)几何中是很有用的。



data_xx.itertuples()

![1566007085944](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566007085944.png)

相关代码:

```python
ratings.groupby('userId') #返回groupby对象
ratings.groupby('userId').any() #聚合 

ratings.groupby('userId').agg()
    
np.random.shuffle(index)    # 打乱列表
```



```python
语法 zip([iterable, ...])

示例：

a = [1,2,3]
b = [4,5,6]
c = [4,5,6,7,8]
zipped = zip(a,b)     # 返回一个对象
>>> zipped
<zip object at 0x103abc288>
>>> list(zipped)  # list() 转换为列表
[(1, 4), (2, 5), (3, 6)]
>>> list(zip(a,c))              # 元素个数与最短的列表一致
[(1, 4), (2, 5), (3, 6)]

 # 与 zip 相反，zip(*) 可理解为解压，返回二维矩阵式
a1, a2 = zip(*zip(a,b))         
>>> list(a1)
[1, 2, 3]
>>> list(a2)
[4, 5, 6]
```



公式推导:

![1566048418609](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566048418609.png)

核心代码:

```python
#更新bu,bi
#number_epochs 迭代次数,alpha学习率,reg正则化系统
for i in range(number_epochs):
    print('inter%d'%i)
    for uid,iid,real_rating in dataset.itertuples(index=False):
        #差值(样本损失)  error = 真实值 - 预测值
        error =real_rating -(global_mean +bu[uid] +bi[iid])
        # 梯度下降法推导
        # bu  = bu+α∗(∑u,i∈R(rui−μ−bu−bi)−λ∗bu) 
        # 随机梯度下降
        # bu = bu + a*(error - λ∗bu)
        bu[uid] += alpha *(error -reg*bu[uid])
        bi[iid] += alpha *(error -reg*bi[iid])
```





##### 1-1随机梯度下降优化

```python
import pandas as pd
import numpy as np

def data_split(data_path, x=0.8, random=False):
    '''
    切分数据集， 这里为了保证用户数量保持不变，将每个用户的评分数据按比例进行拆分
    :param data_path: 数据集路径
    :param x: 训练集的比例，如x=0.8，则0.2是测试集
    :param random: 是否随机切分，默认False
    :return: 用户-物品评分矩阵
    '''
    print("开始切分数据集...")
    # 设置要加载的数据字段的类型
    dtype = {"userId": np.int32, "movieId": np.int32, "rating": np.float32}
    # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分
    ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3))

    testset_index = []
    # 为了保证每个用户在测试集和训练集都有数据，因此按userId聚合
    for uid in ratings.groupby("userId").any().index:
        user_rating_data = ratings.where(ratings["userId"]==uid).dropna()
        if random:
            # 因为不可变类型不能被 shuffle方法作用，所以需要强行转换为列表
            index = list(user_rating_data.index)
            np.random.shuffle(index)    # 打乱列表
            _index = round(len(user_rating_data) * x)
            testset_index += list(index[_index:])
        else:
            # 将每个用户的x比例的数据作为训练集，剩余的作为测试集
            index = round(len(user_rating_data) * x)
            testset_index += list(user_rating_data.index.values[index:])

    testset = ratings.loc[testset_index]
    trainset = ratings.drop(testset_index)
    print("完成数据集切分...")
    return trainset, testset

def accuray(predict_results, method="all"):
    '''
    准确性指标计算方法
    :param predict_results: 预测结果，类型为容器，每个元素是一个包含uid,iid,real_rating,pred_rating的序列
    :param method: 指标方法，类型为字符串，rmse或mae，否则返回两者rmse和mae
    :return:
    '''

    def rmse(predict_results):
        '''
        rmse评估指标
        :param predict_results:
        :return: rmse
        '''
        length = 0
        _rmse_sum = 0
        for uid, iid, real_rating, pred_rating in predict_results:
            length += 1
            _rmse_sum += (pred_rating - real_rating) ** 2
        return round(np.sqrt(_rmse_sum / length), 4)

    def mae(predict_results):
        '''
        mae评估指标
        :param predict_results:
        :return: mae
        '''
        length = 0
        _mae_sum = 0
        for uid, iid, real_rating, pred_rating in predict_results:
            length += 1
            _mae_sum += abs(pred_rating - real_rating)
        return round(_mae_sum / length, 4)

    def rmse_mae(predict_results):
        '''
        rmse和mae评估指标
        :param predict_results:
        :return: rmse, mae
        '''
        length = 0
        _rmse_sum = 0
        _mae_sum = 0
        for uid, iid, real_rating, pred_rating in predict_results:
            length += 1
            _rmse_sum += (pred_rating - real_rating) ** 2
            _mae_sum += abs(pred_rating - real_rating)
        return round(np.sqrt(_rmse_sum / length), 4), round(_mae_sum / length, 4)

    if method.lower() == "rmse":
        rmse(predict_results)
    elif method.lower() == "mae":
        mae(predict_results)
    else:
        return rmse_mae(predict_results)
#随机梯度下降 封装
class BaselineCFBySGD(object):
    def __init__(self,number_epochs,alpha,reg,columns=['uid','iid','rating']):
        
        # 梯度下降最高迭代次数
        self.number_epochs = number_epochs
        # 学习率
        self.alpha = alpha
        # 正则参数
        self.reg = reg
        # 数据集中user-item-rating字段的名称
        self.columns = columns
        
    
    def fit(self,dataset):
        self.dataset = dataset
        #用户评分数据
        self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]]
        #用户评分数据
        self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]]
        # 计算全局平均分
        self.global_mean = self.dataset[self.columns[2]].mean()
        # 调用sgd方法训练模型参数
        self.bu, self.bi = self.sgd()
        
    def sgd(self):
                '''
        利用随机梯度下降，优化bu，bi的值
        :return: bu, bi
        
        '''
        # 初始化bu、bi的值，全部设为0
        bu = dict(zip(self.users_ratings.index, np.zeros(len(self.users_ratings))))
        bi = dict(zip(self.items_ratings.index, np.zeros(len(self.items_ratings))))
        
        #更新bu,bi
        #number_epochs 迭代次数,alpha学习率,reg正则化系统
        for i in range(number_epochs):
            print('inter%d'%i)
            for uid,iid,real_rating in dataset.itertuples(index=False):
                #差值(样本损失)  error = 真实值 - 预测值
                error =real_rating -(global_mean +bu[uid] +bi[iid])
                # 梯度下降法推导
                # bu  = bu+α∗(∑u,i∈R(rui−μ−bu−bi)−λ∗bu) 
                # 随机梯度下降
                # bu = bu + a*(error - λ∗bu)
                bu[uid] += alpha *(error -reg*bu[uid])
                bi[iid] += alpha *(error -reg*bi[iid])
        return bu,bi
    
    #预测
    def predict(self,uid,iid):
        predict_rating =self.global_mean + self.bu[uid] + self.bi[iid]
        
        return predict_rating
    

#调用
if __name__ == '__main__':
    trainset, testset = data_split("../../data/ml-latest-small/ratings.csv", random=True)

    bcf = BaselineCFBySGD(20, 0.1, 0.1, ["userId", "movieId", "rating"])
    bcf.fit(trainset)

    pred_results = bcf.test(testset)

    rmse, mae = accuray(pred_results)

    print("rmse: ", rmse, "mae: ", mae)
```







针对回归问题的评估:均方根误差rmse,平均绝对误差mae



##### 1-2 ALS交替最小二乘法优化

算某个系数时,默认另外为已知

![1566010733076](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566010733076.png)



![1566011166825](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566011166825.png)

- 迭代更新bu bi

```python
for i in range(number_epochs):
    print("iter%d" % i)
    for iid, uids, ratings in items_ratings.itertuples(index=True):
        _sum = 0
        for uid, rating in zip(uids, ratings):
            _sum += rating - global_mean - bu[uid]
        bi[iid] = _sum / (reg_bi + len(uids))

    for uid, iids, ratings in users_ratings.itertuples(index=True):
        _sum = 0
        for iid, rating in zip(iids, ratings):
            _sum += rating - global_mean - bi[iid]
        bu[uid] = _sum / (reg_bu + len(iids))
```





#### 2-基于矩阵分解的协同过滤推荐 !!

##### Surprise库

Surprise · A Python scikit for recommender systems.

<http://surpriselib.com/>

<https://github.com/NicolasHug/Surprise>

![1566022898501](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566022898501.png)





##### Traditional SVD

![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day02_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/img/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A31.jpg)

 SVD 适用于稠密矩阵,评分系统一般为稀疏矩阵,即不适合用SVD



##### FunkSVD (LFM)

适用于**稀疏矩阵**

用户隐语义模型矩阵

- 用户向量

物品隐主义模型矩阵

- 物品向量

在**spark**中有该功能的封装



刚才提到的Traditional SVD首先需要填充矩阵，然后再进行分解降维，同时存在计算复杂度高的问题，因为要分解成3个矩阵，所以后来提出了Funk SVD的方法，它不在将矩阵分解为3个矩阵，而是分解为2个用户-**隐含特征**，项目-隐含特征的矩阵，Funk SVD也被称为最原始的LFM模型

![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day02_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/img/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A32.jpg)

借鉴线性回归的思想，通过最小化观察数据的平方来寻求最优的用户和项目的隐含向量表示。同时为了避免过度拟合（Overfitting）观测数据，又提出了带有L2正则项的FunkSVD，上公式：

![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day02_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/img/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A33.jpg)

以上两种最优化函数都可以通过梯度下降或者随机梯度下降法来寻求最优解。

![1566012662887](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566012662887.png)



梯度下降算法[尚硅谷]

![1566015861700](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566015861700.png)

##### LFM梯度下降代码实现

```python
# 评分矩阵R
R = np.array([[4,0,2,0,1],
             [0,2,3,0,0],
             [1,0,2,4,0],
             [5,0,0,3,1],
             [0,0,1,5,1],
             [0,3,2,4,1],])

"""
@输入参数：
R：M*N 的评分矩阵
K：隐特征向量维度
max_iter: 最大迭代次数
alpha：步长
lamda：正则化系数
@输出：
分解之后的 P，Q
P：初始化用户特征矩阵M*K
Q：初始化物品特征矩阵N*K
"""

# 给定超参数
K = 5
max_iter = 5000
alpha = 0.0002
lamda = 0.004

# 核心算法
def LFM_grad_desc( R, K=2, max_iter=1000, alpha=0.0001, lamda=0.002 ):
    # 基本维度参数定义
    M = len(R)
    N = len(R[0])
    
    # P,Q初始值，随机生成
    P = np.random.rand(M, K)
    Q = np.random.rand(N, K)
    Q = Q.T #转置
    
    # 开始迭代
    for step in range(max_iter):
        # 对所有的用户u、物品i做遍历，对应的特征向量Pu、Qi梯度下降
        for u in range(M):
            for i in range(N):
                # 对于每一个大于0的评分，求出预测评分误差
                if R[u][i] > 0:
                    eui = np.dot( P[u,:], Q[:,i] ) - R[u][i]
                    
                    # 代入公式，按照梯度下降算法更新当前的Pu、Qi
                    for k in range(K):
                        P[u][k] = P[u][k] - alpha * ( 2 * eui * Q[k][i] + 2 * lamda * P[u][k] )
                        Q[k][i] = Q[k][i] - alpha * ( 2 * eui * P[u][k] + 2 * lamda * Q[k][i] )
        
        # u、i遍历完成，所有特征向量更新完成，可以得到P、Q，可以计算预测评分矩阵
        predR = np.dot( P, Q )
        
        # 计算当前损失函数
        cost = 0
        for u in range(M):
            for i in range(N):
                if R[u][i] > 0:
                    cost += ( np.dot( P[u,:], Q[:,i] ) - R[u][i] ) ** 2
                    # 加上正则化项
                    for k in range(K):
                        cost += lamda * ( P[u][k] ** 2 + Q[k][i] ** 2 )
        if cost < 0.0001:
            break
        
    return P, Q.T, cost


#测试
P, Q, cost = LFM_grad_desc(R, K, max_iter, alpha, lamda)

print(P)
print(Q)
print(cost)

predR = P.dot(Q.T)

print(R)
print(predR)
```



黑马代码LFM

```python
#数据加载
import pandas as pd
import numpy as np


dtype = [("userId", np.int32), ("movieId", np.int32), ("rating", np.float32)]
dataset = pd.read_csv("ml-latest-small/ratings.csv", usecols=range(3), dtype=dict(dtype))

#数据初始化

# 用户评分数据  groupby 分组  groupby('userId') 根据用户id分组 agg（aggregation聚合）
users_ratings = dataset.groupby('userId').agg([list])
# 物品评分数据
items_ratings = dataset.groupby('movieId').agg([list])
# 计算全局平均分
global_mean = dataset['rating'].mean()
# 初始化P Q  610  9700   K值  610*K    9700*K
# User-LF  10 代表 隐含因子个数是10个
P = dict(zip(users_ratings.index,np.random.rand(len(users_ratings),10).astype(np.float32)
        ))
# Item-LF
Q = dict(zip(items_ratings.index,np.random.rand(len(items_ratings),10).astype(np.float32)
        ))

#梯度下降优化损失函数
for i in range(15):
    print('*'*10,i)
    for uid,iid,real_rating in dataset.itertuples(index = False):
        #遍历 用户 物品的评分数据 通过用户的id 到用户矩阵中获取用户向量
        v_puk = P[uid]
        # 通过物品的uid 到物品矩阵里获取物品向量
        v_qik = Q[iid]
        #计算损失
        error = real_rating-np.dot(v_puk,v_qik)
        # 0.02学习率 0.01正则化系数
        v_puk += 0.02*(error*v_qik-0.01*v_puk)
        v_qik += 0.02*(error*v_puk-0.01*v_qik)

        P[uid] = v_puk
        Q[iid] = v_qik
#评分预测
def predict(self, uid, iid):
    # 如果uid或iid不在，我们使用全剧平均分作为预测结果返回
    if uid not in self.users_ratings.index or iid not in self.items_ratings.index:
        return self.globalMean
    p_u = self.P[uid]
    q_i = self.Q[iid]

    return np.dot(p_u, q_i)
'''
LFM Model
'''

# 评分预测    1-5
class LFM(object):

    def __init__(self, alpha, reg_p, reg_q, number_LatentFactors=10, number_epochs=10, columns=["uid", "iid", "rating"]):
        self.alpha = alpha # 学习率
        self.reg_p = reg_p    # P矩阵正则
        self.reg_q = reg_q    # Q矩阵正则
        self.number_LatentFactors = number_LatentFactors  # 隐式类别数量
        self.number_epochs = number_epochs    # 最大迭代次数
        self.columns = columns

    def fit(self, dataset):
        '''
        fit dataset
        :param dataset: uid, iid, rating
        :return:
        '''

        self.dataset = pd.DataFrame(dataset)

        self.users_ratings = dataset.groupby(self.columns[0]).agg([list])[[self.columns[1], self.columns[2]]]
        self.items_ratings = dataset.groupby(self.columns[1]).agg([list])[[self.columns[0], self.columns[2]]]

        self.globalMean = self.dataset[self.columns[2]].mean()

        self.P, self.Q = self.sgd()

    def _init_matrix(self):
        '''
        初始化P和Q矩阵，同时为设置0，1之间的随机值作为初始值
        :return:
        '''
        # User-LF
        P = dict(zip(
            self.users_ratings.index,
            np.random.rand(len(self.users_ratings), self.number_LatentFactors).astype(np.float32)
        ))
        # Item-LF
        Q = dict(zip(
            self.items_ratings.index,
            np.random.rand(len(self.items_ratings), self.number_LatentFactors).astype(np.float32)
        ))
        return P, Q

    def sgd(self):
        '''
        使用随机梯度下降，优化结果
        :return:
        '''
        P, Q = self._init_matrix()

        for i in range(self.number_epochs):
            print("iter%d"%i)
            error_list = []
            for uid, iid, r_ui in self.dataset.itertuples(index=False):
                # User-LF P
                ## Item-LF Q
                v_pu = P[uid] #用户向量
                v_qi = Q[iid] #物品向量
                err = np.float32(r_ui - np.dot(v_pu, v_qi))

                v_pu += self.alpha * (err * v_qi - self.reg_p * v_pu)
                v_qi += self.alpha * (err * v_pu - self.reg_q * v_qi)

                P[uid] = v_pu 
                Q[iid] = v_qi

                # for k in range(self.number_of_LatentFactors):
                #     v_pu[k] += self.alpha*(err*v_qi[k] - self.reg_p*v_pu[k])
                #     v_qi[k] += self.alpha*(err*v_pu[k] - self.reg_q*v_qi[k])

                error_list.append(err ** 2)
            print(np.sqrt(np.mean(error_list)))
        return P, Q

    def predict(self, uid, iid):
        # 如果uid或iid不在，我们使用全剧平均分作为预测结果返回
        if uid not in self.users_ratings.index or iid not in self.items_ratings.index:
            return self.globalMean

        p_u = self.P[uid]
        q_i = self.Q[iid]

        return np.dot(p_u, q_i)

    def test(self,testset):
        '''预测测试集数据'''
        for uid, iid, real_rating in testset.itertuples(index=False):
            try:
                pred_rating = self.predict(uid, iid)
            except Exception as e:
                print(e)
            else:
                yield uid, iid, real_rating, pred_rating

if __name__ == '__main__':
    dtype = [("userId", np.int32), ("movieId", np.int32), ("rating", np.float32)]
    dataset = pd.read_csv("datasets/ml-latest-small/ratings.csv", usecols=range(3), dtype=dict(dtype))

    lfm = LFM(0.02, 0.01, 0.01, 10, 100, ["userId", "movieId", "rating"])
    lfm.fit(dataset)

    while True:
        uid = input("uid: ")
        iid = input("iid: ")
        print(lfm.predict(int(uid), int(iid)))
```





##### LFM总结

LFM(FunkSVD)实现流程(重点)

- LFM(FunkSVD)梯度下降推导
  - 利用平方差来构建损失函数, 加入正则化项
  - 梯度下降更新参数 puk和qik
    - 损失函数: 分别对puk和qik求偏导,
    - 随机梯度, 去掉最外部的求和, 得出puk和qik梯度下降公式
- 实现思路:
  - 加载数据
  - 数据初始
  - 梯度下降优化损失函数(更新puk, qik的值)
  - 评分预测

- LFM(FunkSVD)基本原理
  - 将用户-物品评分矩阵拆分为两个小矩阵, 用户隐因子矩阵, 物品隐因子矩阵
  - 用户隐因子矩阵(P) 每一个维度代表了会影响到用户对物品评分的用户特征
  - 物品隐因子矩阵(Q) 每一个维度代表了会影响到物品得分的特征
  - 从用户隐因子矩阵中取出用户向量, 从物品隐因子矩阵矩阵中取出物品向量, 点积即可得到用户对该物品的评分预测
  - 利用优化损失函数的思想求解两个矩阵, 可以采用梯度下降的方法
- LFM(FunkSVM)的梯度下降优化
  - 初始化两个矩阵P和Q, 隐因子个数需要人为设定
  - 向量相乘使用numpy的dot函数直接计算



##### BiasSVD:

在LFM加入Baseline思想 ,引入用户偏置,物品偏置

在FunkSVD提出来之后，出现了很多变形版本，其中一个相对成功的方法是BiasSVD，顾名思义，即带有偏置项的SVD分解：

![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day02_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/img/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A34.jpg)

它基于的假设和Baseline基准预测是一样的，但这里将Baseline的偏置引入到了矩阵分解中







##### SVD++:

人们后来又提出了改进的BiasSVD，被称为SVD++，该算法是在BiasSVD的基础上添加了用户的隐式反馈信息：

![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day02_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/img/%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A35.jpg)

显示反馈指的用户的评分这样的行为，隐式反馈指用户的浏览记录、购买记录、收听记录等。

SVD++是基于这样的假设：在BiasSVD基础上，认为用户对于项目的历史浏览记录、购买记录、收听记录等可以从侧面反映用户的偏好。



##### 协同过滤VS隐语义

![1565938142157](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1565938142157.png)

##### 隐语义

即基于模型的协同过滤 ModelBasedCF

隐含因子

![1566034224944](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566034224944.png)

##### 评估标准

- 准确度

- 召回率
- 覆盖率
- 多样性

![1566036786318](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566036786318.png)



### 基于内容的推荐算法 CB

（Content-Based）

#### 简介

在发现用户新兴趣方面不如协同过滤(CF)



物品打标签,物品画像

有用户行为数据

- 用户行为画像
- 建立标签对物品的倒排索引
- 用户画像标签找对应的物品
  - 根据用户对标签的消费次数
  - 用户对标签消费时的评分

可以解决冷启动

![1565925726385](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1565925726385.png)



基于内容的推荐方法是非常直接的，它以物品的内容描述信息为依据来做出的推荐，本质上是基于对物品和用户自身的特征或属性的直接分析和计算。

例如，假设已知电影A是一部喜剧，而恰巧我们得知某个用户喜欢看喜剧电影，那么我们基于这样的已知信息，就可以将电影A推荐给该用户。



##### 基于内容的推荐

- PGC生成标签

- 从描述性的文字中提取关键词
  - Tf_idf    , textrank
- word2vec 词向量
  - 词-->向量
- doc2vec
  - 文档-->向量





##### 算法流程：

- 建立物品画像
- 有用户行为数据
  - 建立用户画像
    - 用户消费过哪些物品 这些物品的标签就可以打到用户身上
  - 建立倒排索引
    - 物品找标签 倒排就是根据标签找物品
  - 根据用户的画像中的标签 找到标签对应的所有物品
    - 排序可以根据用户对标签的消费次数
    - 也可以根据用户对标签的打分情况
- 没有用户行为数据情况 系统冷启动
  - 建立物品画像 系统生成
  - 从描述性的文字中提取关键词
    - tf-idf textrank
  - word2vec/doc2vec
  - 把物品的描述-》转换成一组关键词-》转换成一个文档向量
  - 文档向量的相似度 就可以表示内容的相似性
- 可以为每一件商品创建文档向量 计算和其它物品的相似度 召回相似度最高的前n个商品

--------

- 根据PGC/UGC内容构建物品画像
  - 建立倒排索引,可以根据标签找物品
- 根据用户行为记录生成用户画像
- 根据用户画像从物品中寻找最匹配的TOP-N物品进行推荐



##### 物品冷启动处理：

没用用户行为数据情况,系统冷启动

- 根据PGC内容构建物品画像
- 利用物品画像计算物品间两两相似情况
- 为每个物品产生TOP-N最相似的物品进行相关推荐：如与该商品相似的商品有哪些？与该文章相似文章有哪些？



#### 用户画像

- 可以用于风控,减少羊毛党的推荐,减少对差评党的推荐
- 标签可视化
- 用户画像可视化
- eg.年度账单总结
- eg.芝麻分维度分析

![1566033790783](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566033790783.png)

![1566037122047](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566037122047.png)


##### 标签体系

![1566037145141](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566037145141.png)

##### 验证

![1566037593117](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566037593117.png)

- 准确性验证

![1566037635892](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566037635892.png)

##### 用户画像生产和应用:逻辑架构

![1566038052237](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566038052237.png)

##### 企业用户触点

![1566038185738](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566038185738.png)





#### 内容推荐算法的原理:

1. 将产品分解为一系列标签。例如,一个手机产品的标签可以包括品牌、价格、产地、颜色、款式等。如果是自营b2c电商,一般可以在产品入库时手动打标签。

   2.基于用户行为(浏览、购买、收藏)计算每个用户的产品兴趣标签。例如,用户购买了一个产品,则将该产品的所有标签赋值给该用户,每个标签打分为1;用户浏览了一个产品,则将该产品的所有标签赋值给该用户,每个标签打分为0.5。计算复杂度为:已有产品数量*用户量。该过程为离线计算。

3. 针对所有新产品,分别计算每个用户的产品标签与每个新产品的相似度(基于cosine similarity)。计算复杂度为:新产品数量*用户量。该过程为在线计算。



从可行性角度,一个应用场景**是否适合**用内容推荐算法取决于:

1. 是否可以持续为产品打标签。

2. 标签是否可以覆盖产品的核心属性?例如,手机产品的标签一般可以覆盖消费者购物的核心决策因素,但是女装一般比较难(视觉效果很难被打标)。

##### **内容推荐算法的优势:**

1. 推荐结果可理解:不仅每个用户的核心兴趣点可以被标签化(便于理解每个用户的兴趣),并且可以在每一个推荐结果的展示中现实标签,便于消费者理解推荐结果(如下图红框)。



![img](https:////upload-images.jianshu.io/upload_images/14262012-5a1ae3b64cc25120?imageMogr2/auto-orient/strip%7CimageView2/2/w/267/format/webp)



2. 推荐结果稳定性强:对于用户行为不丰富的产品类型(例如,金融产品),协同过滤很难找到同兴趣用户群或关联产品,在相似度计算中稀疏度太高。然而,内容推荐主要使用标签,标签对用户兴趣捕捉稳定性要远远高于单个产品。

3. 便于人机协作:用户可以勾选或者关注推荐标签,从而通过自己的操作来发现自己的个性化需求。



##### 内容推荐算法的劣势:

1. 不适合发现惊喜:如果一个产品不易于被标签穷举或描述产品的标签还没出现,则该产品很难被准确推荐。

2. 在线应用计算复杂度较高:需要基于每个用户来计算相似产品。



##### 基于内容推荐(CB) 与 Item_CF 的区别

向量来源不同:

Item_CF 需要用户对物品评分的数据

CB 通过文本检索技术把词/文档转成向量,可以不需要用户参与,即不受限用户行为数据

CB基于文本分析

- 基于内容推荐解决系统冷启动问题套路
  - 文本检索技术 把词/文档转换成了向量
  - 物品-》向量描述-》计算向量的相似度-》把和当前物品相似度高的内容推荐出去
  - 不受限与用户行为数据
- 协同过滤 基于物品的协同过滤
  - 用户-物品 评分矩阵 -> 向量描述
  - 物品-》向量描述-》计算向量的相似度-》把和当前物品相似度高的内容推荐出去







### Hadoop

- 重点

- HDFS的使用 ☆☆☆☆☆
  - 启动
  - 文件的上传下载删除
- MapReduce的原理 ☆☆☆☆
  - Map
  - Reduce
- MRJob☆☆☆
  - python开发MapReduce
  - 继承MRJob  mapper  reducer
  - 练习WordCount案例
- HDFS的架构以及读写流程
  - NameNode
  - DataNode



#### Hadoop的概念

- Apache™ Hadoop® 是一个开源的,可靠的(reliable),可扩展的(scalable)分布式计算框架
  - 分布式计算框架:允许使用简单的编程模型跨计算机集群分布式处理大型数据集
  - **可扩展**: 从单个服务器扩展到数万台计算机，每台计算机都提供本地计算和存储
  - **可靠的**: 不依靠硬件来提供高可用性(high-availability)，而是在应用层检测和处理故障，从而在计算机集群之上提供高可用服务

##### Hadoop用途

- 搭建大型数据仓库
  - 保存数据的历史版本
- PB级数据的存储 处理 分析 统计等业务
  - 搜索引擎
  - 日志分析(日活,消量,新增用户,7日留存,月留存)
  - 数据挖掘(挖掘=分析 + 预测)
  - 商业智能(Business Intelligence，简称：BI)

##### 发展史

- 2003-2004年 Google发表了三篇论文
  - GFS：Google的分布式文件系统Google File System
  - [MapReduce](https://en.wikipedia.org/wiki/MapReduce): Simplified Data Processing on Large Clusters
  - BigTable：一个大型的分布式数据库
- 2006年2月Hadoop成为Apache的独立开源项目( Doug Cutting等人实现了DFS和MapReduce机制)。

- 
- 搜索引擎时代
  - 有保存大量网页的需求(单机 集群)
  - 词频统计 word count PageRank
- 数据仓库时代
  - FaceBook推出**Hive**
  - 曾经进行数分析与统计时, 仅限于数据库,受数据量和计算能力的限制, 我们只能对最重要的数据进行统计和分析(决策数据,财务相关)
  - Hive可以在Hadoop上运行SQL操作, 可以把运行日志, 应用采集数据,数据库数据放到一起分析
- 数据挖掘时代
  - 啤酒尿不湿
  - 关联分析
  - 用户画像/物品画像
- 机器学习时代 广义大数据
  - 大数据提高数据存储能力, 为机器学习提供燃料
  - alpha go
  - siri 小爱同学 天猫精灵

#####  Hadoop优势

- 高可靠
  - 数据存储: 数据块多副本
  - 数据计算: 某个节点崩溃, 会自动重新调度作业计算
- 高扩展性
  - 存储/计算资源不够时，可以横向的线性扩展机器
  - 一个集群中可以包含数以千计的节点
  - 集群可以使用廉价机器，成本低
- Hadoop生态系统成熟
  - 有开源的功能模块,改装一下就能应用





##### Hadoop组件

- MapReduce
- YARN
- HDFS



#### MapReduce分布式计算框架

A YARN-based system for parallel processing of large data sets.

- 分布式**计算框架**,Map拆分数据并计算,Reduce汇整每一部分的计算结果
- 源于Google的MapReduce论文，论文发表于2004年12月
- MapReduce是GoogleMapReduce的开源实现
- MapReduce特点:扩展性&容错性&海量数据离线处理
- 移动计算比移动数据要划算



##### mrjob 简介

- 使用python开发在Hadoop上运行的程序, mrjob是最简单的方式
- mrjob程序可以在本地测试运行也可以部署到Hadoop集群上运行
- 如果不想成为hadoop专家, 但是需要利用Hadoop写MapReduce代码,mrJob是很好的选择



##### MRJob实现MapReduce

- hadoop 提供了一个hadoop streaming的jar包， 通过hadoop streaming 可以用python 脚本写mapreduce任务 ， hadoop streaming 做用帮助把脚本翻译成java, 使用hadoop streaming有些麻烦
  - map阶段对应一个python文件
  - reduce阶段对应一个python文件
- MRJob用法
  - 创建一个类继承MRJob
  - 重写 mapper 和 reducer
  - 如果有多个map 和reduce 阶段 需要创建MRStep对象
  - 创建MRStep对象 可以指定每一个阶段的mapper对应的方法，reducer对应的方法，combiner对应的方法
  - 通过重写steps方法 返回MRStep的list 指定多个step的执行顺序



##### mrjob实现WordCount

```python
from mrjob.job import MRJob

class MRWordCount(MRJob):

    #每一行从line中输入
    def mapper(self, _, line):
        for word in line.split():
            yield word,1

    # word相同的 会走到同一个reduce
    def reducer(self, word, counts):
        yield word, sum(counts)

if __name__ == '__main__':
    MRWordCount.run()
```

##### 运行WordCount代码
```shell
#打开命令行, 找到一篇文本文档, 敲如下命令:
python mr_word_count.py my_file.txt


#运行MRJOB的不同方式
#1-Hadoop(-r hadoop)方式
#用于hadoop环境，支持Hadoop运行调度控制参数，如：

#1)指定Hadoop任务调度优先级(VERY_HIGH|HIGH),如：--jobconf mapreduce.job.priority=VERY_HIGH

#2)Map及Reduce任务个数限制，如：--jobconf mapreduce.map.tasks=2 --jobconf mapreduce.reduce.tasks=5

python word_count.py -r hadoop hdfs:///test.txt -o hdfs:///output
```







##### mrjob实现topN统计

```python
import sys
from mrjob.job import MRJob,MRStep
import heapq

class TopNWords(MRJob):
    def mapper(self, _, line):
        if line.strip() != "":
            for word in line.strip().split():
                yield word,1

    #介于mapper和reducer之间，用于临时的将mapper输出的数据进行统计
    def combiner(self, word, counts):
        yield word,sum(counts)

    def reducer_sum(self, word, counts):
        yield None,(sum(counts),word)

    #利用heapq将数据进行排序，将最大的2个取出
    def top_n_reducer(self,_,word_cnts):
        for cnt,word in heapq.nlargest(2,word_cnts):
            yield word,cnt

    #实现steps方法用于指定自定义的mapper，comnbiner和reducer方法
    def steps(self):
        #传入两个step 定义了执行的顺序
        return [
            MRStep(mapper=self.mapper,
                   combiner=self.combiner,
                   reducer=self.reducer_sum),
            MRStep(reducer=self.top_n_reducer)
        ]

def main():
    TopNWords.run()

if __name__=='__main__':
    main()
```

![1566119723088](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566119723088.png)





##### MapReduce原理

**单机程序计算流程**

输入数据--->读取数据--->处理数据--->写入数据--->输出数据

**Hadoop计算流程**

input data：输入数据

InputFormat：对数据进行切分，格式化处理

map：将前面切分的数据做map处理(将数据进行分类，输出(k,v)键值对数据)

shuffle&sort:将相同的数据放在一起，并对数据进行排序处理

reduce：将map输出的数据进行hash计算，对每个map数据进行统计计算

OutputFormat：格式化输出数据





![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day03_Hadoop/img/mp3.png)



![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day03_Hadoop/img/mp1.png)

##### MapReduce架构

MapReduce架构 1.X

- JobTracker:负责接收客户作业提交，负责任务到作业节点上运行，检查作业的状态
- TaskTracker：由JobTracker指派任务，定期向JobTracker汇报状态，在每一个工作节点上永远只会有一个TaskTracker

MapReduce2.X架构

- ResourceManager：负责资源的管理，负责提交任务到NodeManager所在的节点运行，检查节点的状态
- NodeManager：由ResourceManager指派任务，定期向ResourceManager汇报状态



##### 计算速度慢的原因

- 进程模型,启动任务需启动JVM虚拟机,比较耗时

- 没有完全使用内存,会用磁盘读写



#### YARN资源调度系统

A framework for job scheduling and cluster resource management.(资源调度系统)

- YARN: Yet Another Resource Negotiator
  - **Hadoop 2.X以前没有YARN,使用Mesos开源框架实现**
- 负责整个集群资源的管理和调度
- YARN特点:扩展性&容错性&多框架资源统一调度



启动MapReduce需要先启动YARN



##### YARN架构

- ResourceManager: RM 资源管理器  整个集群同一时间提供服务的RM只有一个，负责集群资源的统一管理和调度  处理客户端的请求： submit, kill  监控我们的NM，一旦某个NM挂了，那么该NM上运行的任务需要告诉我们的AM来如何进行处理
  - 同一时间只能有一个
- NodeManager: NM 节点管理器  整个集群中有多个，负责自己本身节点资源管理和使用  定时向RM汇报本节点的资源使用情况  接收并处理来自RM的各种命令：启动Container  处理来自AM的命令
  - 同一时间可以有多个运行
- ApplicationMaster: AM  每个应用程序对应一个：MR、Spark，负责应用程序的管理  为应用程序向RM申请资源（core、memory），分配给内部task  需要与NM通信：启动/停止task，task是运行在container里面，AM也是运行在container里面
- Container 容器: 封装了CPU、Memory等资源的一个容器,是一个任务运行环境的抽象
- Client: 提交作业 查询作业的运行进度,杀死作业





##### 流程

![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day03_Hadoop/img/yarn4.png)





1，Client提交作业请求

2，ResourceManager 进程和 NodeManager 进程通信，根据集群资源，为用户程序分配第一个Container(容器)，并将 ApplicationMaster 分发到这个容器上面

3，在启动的Container中创建ApplicationMaster

4，ApplicationMaster启动后向ResourceManager注册进程,申请资源

5，ApplicationMaster申请到资源后，向对应的NodeManager申请启动Container,将要执行的程序分发到NodeManager上

6，Container启动后，执行对应的任务

7，Tast执行完毕之后，向ApplicationMaster返回结果

8，ApplicationMaster向ResourceManager 请求kill





#### HDFS分布式文件系统

##### Hadoop Distributed File System (HDFS)

 A distributed file system that provides high-throughput access to application data.(分布式文件系统)

- 源自于Google的GFS论文, 论文发表于2003年10月
- HDFS是GFS的开源实现
- HDFS的特点:扩展性&容错性&海量数量存储
- 将文件切分成指定大小的数据块, 并在多台机器上保存多个副本
- 数据切分、多副本、容错等操作对用户是透明的



##### 命令

| 命令                     | 说明                                          |
| ------------------------ | --------------------------------------------- |
| hadoop fs -mkdir         | 创建HDFS目录                                  |
| hadoop fs -ls            | 列出HDFS目录                                  |
| hadoop fs -copyFromLocal | 使用-copyFromLocal复制本地文件（local）到HDFS |
| hadoop fs -put           | 使用-put复制本地（local）文件到HDFS           |
| hadoop fs -copyToLocal   | 将HDFS上的文件复制到本地（local）             |
| hadoop fs -get           | 将HDFS上的文件复制到本地（local）             |
| hadoop fs -cp            | 复制HDFS文件                                  |
| hadoop fs -rm            | 删除HDFS文件                                  |
| hadoop fs -cat           | 列出HDFS目录下的文件的内容                    |

hadoop fs -put localfile /user/hadoop/hadoopfile

hadoop fs -ls /   #显示hadoop根目录

#删除

hadoop fs -rmr xxxx       hadoop fs -rm-r xxxx



- 防火墙:
  - 关闭防火墙: systemctl stop firewalld
  - 查看防火墙命令: systemctl status firewalld
  - 禁用防火墙自启命令: systemctl disable firewalld
  - 启动防火墙：systemctl start firewalld.service
  - 启用防火墙自启命令: systemctl enable firewalld.service
- 退出安全模式
  - hdfs dfsadmin -safemode leave
  - 或
  - hadoop dfsadmin -safemode leave



##### HDFS的设计目标

- 适合运行在通用硬件(commodity hardware)上的分布式文件系统
- 高度容错性的系统，适合部署在廉价的机器上
- HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用
- 容易扩展，为用户提供性能不错的文件存储服务

##### HDFS架构

心跳机制,哨兵机制

- 1个NameNode/NN(Master) 带 DataNode/DN(Slaves) (Master-Slave结构)
- 1个文件会被拆分成多个Block
- NameNode(NN)
  - 负责客户端请求的响应
  - 负责元数据（文件的名称、副本系数、Block存放的DN）的管理
    - 元数据 MetaData 描述数据的数据
  - 监控DataNode健康状况 10分钟没有收到DataNode报告认为Datanode死掉了
- DataNode(DN),**心跳机制**
  - 存储用户的文件对应的数据块(Block)
  - 要定期向NN发送心跳信息，汇报本身及其所有的block信息，健康状况
- 分布式集群NameNode和DataNode部署在不同机器上



##### HDFS优缺点

- 优点
  - 数据冗余 硬件容错
  - 适合存储大文件
  - 处理流式数据
  - 可构建在廉价机器上
- 缺点
  - 低延迟的数据访问
  - 小文件存储



##### HDFS环境搭建

由java开发,需要 JDK环境

JDK java的开发运行环境

- 大部分的大数据框架都是用java或者scala开发的
  - scala  Java虚拟机语言
  - java -> .class->.jar   .jar文件就是java虚拟机的可执行文件
  - scala语法和java有区别  .scala -> .class ->.jar 



jps 查看java进程





##### HDFS 读写流程

- 写入 

client--namenode(中介)--datanodes

![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day03_Hadoop/img/hdfs_read_write/a.jpg)



![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day03_Hadoop/img/hdfs_read_write/c.jpg)

- 读

![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day03_Hadoop/img/hdfs_read_write/d.jpg)





- 写流程
  - 客户端负责数据的拆分，拆成128MB一块的小文件
  - NameNode根据设置的副本数量，负责返回要保存的DataNode列表，如果是3副本，每一个block 返回3台DataNode的URL地址
  - DataNode 负责数据的保存，和数据的复制
    - 客户端只需要把数据和列表提交给列表中的第一台机器， DataNode之间数据复制DataNode自己完成
- 读流程
  - 客户端提交文件名给NameNode
  - NameNode返回当前文件对应哪些block,以及每一个block的所有DataNode地址
  - 客户端到地址列表中的第一台DataNode取数据





##### HDFS如何实现高可用(HA)

- 数据存储故障容错
  - 磁盘介质在存储过程中受环境或者老化影响,数据可能错乱
  - 对于存储在 DataNode 上的数据块，计算并存储校验和（CheckSum)
  - 读取数据的时候, 重新计算读取出来的数据校验和, 校验不正确抛出异常, 从其它DataNode上读取备份数据
- 磁盘故障容错
  - DataNode 监测到本机的某块磁盘损坏
  - 将该块磁盘上存储的所有 BlockID 报告给 NameNode
  - NameNode 检查这些数据块在哪些DataNode上有备份,
  - 通知相应DataNode, 将数据复制到其他服务器上
- DataNode故障容错
  - 通过心跳和NameNode保持通讯
  - 超时未发送心跳, NameNode会认为这个DataNode已经宕机
  - NameNode查找这个DataNode上有哪些数据块, 以及这些数据在其它DataNode服务器上的存储情况
  - 从其它DataNode服务器上复制数据
- NameNode故障容错
  - 主从热备 secondary namenode
  - zookeeper配合 master节点选举



##### Hadoop版本选择



#### Hadoop生态圈



##### 广义的Hadoop

- 指Hadoop生态系统，Hadoop生态系统是一个很庞大的概念，hadoop是其中最重要最基础的一个部分，生态系统中每一子系统只解决某一个特定的问题域（甚至可能更窄），不搞统一型的全能系统，而是小而精的多个小系统；

![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/day03_Hadoop/img/hadoop-%E7%94%9F%E6%80%81.png)



##### Hive

数据仓库,SQL Query

- sql操作 MapReduce

- Hive 由 Facebook 实现并开源，是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据映射为一张数据库表，并提供 HQL(Hive SQL)查询功能，底层数据是存储在 HDFS 上。
- Hive 本质: 将 SQL 语句转换为 MapReduce 任务运行，使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据,是一款基于 HDFS 的 MapReduce **计算框架**
- 主要用途：用来做**离线**数据分析，比直接用 MapReduce 开发效率更高。





##### Spark

分布式的计算框架基于内存

- spark core
  - 工具性组件
- spark sql
  - 离线
- spark streaming 
  - 准实时 不算是一个标准的流式计算(实时)
- spark ML (机器学习库)
- spark MLlib (机器学习库)



##### Flink

: 分布式的流式计算框架(阿里)



**Storm**: 分布式的流式计算框架 python操作storm,未提供机器学习模块

Kafka: 消息队列

Mahout:机器学习库

- 基于 java

##### **Sqoop**

数据交换框架，例如：关系型数据库与HDFS之间的数据交换

- 数据交换(mysql-->hdfs)
- 数据导入导出

##### Hbase

nosql ,**列式数据库**,海量数据中的查询，相当于分布式文件系统中的数据库,mysql,redis为行数据库

- 适合简单的表结构,没有join关联

R:数据分析

pig：脚本语言，跟Hive类似

Oozie:工作流引擎，管理作业执行顺序

Zookeeper:用户无感知，主节点挂掉选择从节点作为主的

- 机器协调,管理员
- 保存数据一致性
- 主节点选举

Flume:日志收集框架





##### Hadoop生态系统的特点

- 开源、社区活跃
- 囊括了大数据处理的方方面面
- 成熟的生态圈



##### 方案

HDFS+ HIVE +MapReduce

HDFS＋Spark





#### 互联网大数据平台架构

- 数据采集
  - App/Web 产生的数据&日志同步到大数据系统
  - 数据库同步:Sqoop 日志同步:Flume 打点: Kafka
  - 不同数据源产生的数据质量可能差别很大
    - 数据库 也许可以直接用
    - 日志 爬虫 大量的清洗,转化处理
- 数据处理
  - 大数据存储与计算的核心
  - 数据同步后导入HDFS
  - MapReduce Hive Spark 读取数据进行计算 结果再保存到HDFS
  - MapReduce Hive Spark 离线计算, HDFS 离线存储
    - 离线计算通常针对(某一类别)全体数据, 比如 历史上所有订单
    - 离线计算特点: 数据规模大, 运行时间长
  - 流式计算
    - 淘宝双11 每秒产生订单数 监控宣传
    - Storm(毫秒) SparkStreaming(秒)
- 数据输出与展示
  - HDFS需要把数据导出交给应用程序, 让用户实时展示 ECharts
    - 淘宝卖家量子魔方
  - 给运营和决策层提供各种统计报告, 数据需要写入数据库
    - 很多运营管理人员, 上班后就会登陆后台数据系统
- 任务调度系统
  - 将上面三个部分整合起来



##### 大数据业务流程

![1566260377900](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566260377900.png)

- 数据采集
  - Flume
  - Sqoop
  - Kafka
  - 爬虫
- 数据存储
  - HDFS
  - HBase
  - Mysql
- 数据调度
  - YARN
- 数据计算
  - MayReduce
  - Hive
  - Spark
  - storm(实时计算)
  - Flink
- 数据挖掘(计算 + 预测)
- 应用层
  - ElasticSearch
  - Tensorflow
  - 热门商品
  - 商品推荐
  - TopN分析
  - 黑白名单



##### Java与大数据关系

- Hadoop,Storm基于Java开发
- Spark基于Scala语言,Scala基于Java





#### 大数据应用--数据分析

- 通过数据分析指标监控企业运营状态, 及时调整运营和产品策略,是大数据技术的关键价值之一

- 大数据平台(互联网企业)运行的绝大多数大数据计算都是关于数据分析的

  - 统计指标
  - 关联分析,
  - 汇总报告,

- 运营数据是公司管理的基础

  - 了解公司目前发展的状况
  - 数据驱动运营: 调节指标对公司进行管理

- 运营数据的获取需要大数据平台的支持

  - 埋点采集数据

  - 数据库,日志 三方采集数据

  - 对数据清洗 转换 存储

  - 利用SQL进行数据统计 汇总 分析

  - 得到需要的运营数据报告

##### 运营常用**数据指标**

  - 新增用户数 UG user growth 用户增长

    - 产品增长性的关键指标
    - 新增访问网站(新下载APP)的用户数

  - 用户留存率

    - 用户留存率 = 留存用户数 / 当期新增用户数
    - 3日留存 5日留存 7日留存

  - 活跃用户数

    - 打开使用产品的用户
    - 日活
    - 月活
    - 提升活跃是网站运营的重要目标

  - PV Page View

    - 打开产品就算活跃
    - 打开以后是否频繁操作就用PV衡量, 每次点击, 页面跳转都记一次PV

  - GMV

    - 成交总金额(Gross Merchandise Volume) 电商网站统计营业额, 反应网站应收能力的重要指标
    - GMV相关的指标: 订单量 客单价

  - **转化率**

    转化率 = 有购买行为的用户数 / 总访问用户数



#### Hive数据仓库

##### 为什么使用 Hive

- 直接使用 MapReduce 处理数据所面临的问题：
  - 人员学习成本太高
  - MapReduce 实现复杂查询逻辑开发难度太大
- 使用 Hive
  - 操作接口采用类 SQL 语法，提供快速开发的能力
  - 避免了去写 MapReduce，减少开发人员的学习成本
  - 功能扩展很方便



##### Hive底层执行引擎

- MapReduce
- Tez
- Spark



##### Hive 与传统数据库对比

|              | Hive                              | 关系型数据库           |
| ------------ | --------------------------------- | ---------------------- |
| ANSI SQL     | 不完全支持                        | 支持                   |
| 更新         | INSERT OVERWRITE\INTO TABLE(默认) | UPDATE\INSERT\DELETE   |
| 事务         | 不支持(默认)                      | 支持                   |
| 模式         | 读模式                            | 写模式                 |
| 查询语言     | HQL                               | SQL                    |
| 数据存储     | HDFS                              | Raw Device or Local FS |
| 执行         | MapReduce                         | Executor               |
| 执行延迟     | 高                                | 低                     |
| 子查询       | 只能用在From子句中                | 完全支持               |
| 处理数据规模 | 大                                | 小                     |
| 可扩展性     | 高                                | 低                     |
| 索引         | 0.8版本后加入位图索引             | 有复杂的索引           |



##### 架构图

![1566132602101](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566132602101.png)







![img](file:///G:/python%E5%AD%A6%E4%B9%A0/%E5%BD%92%E6%A1%A3-%E8%AF%BE%E4%BB%B6-%E8%A7%86%E9%A2%91/%E8%AF%BE%E4%BB%B6/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_book/Hive&HBase/img/hive2.jpg)



##### 组件

- 用户接口：包括 CLI、JDBC/ODBC、WebGUI。
  - CLI(command line interface)为 shell 命令行,在终端输入hive
  - JDBC/ODBC 是 Hive 的 JAVA 实现，与传统数据库JDBC 类似
  - WebGUI 是通过浏览器访问 Hive。
  - HiveServer2基于Thrift, 允许远程客户端使用多种编程语言如Java、Python向Hive提交请求
- 元数据存储：通常是存储在关系数据库如 mysql/derby 中。
  - Hive 将元数据存储在数据库中。
  - Hive 中的元数据包括
    - 表的名字
    - 表的列
    - 分区及其属性
    - 表的属性（是否为外部表等）
    - 表的数据所在目录等。
- 解释器、编译器、优化器、执行器:完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行





##### Hive 与 Hadoop 的关系

Hive 利用 HDFS 存储数据，利用 MapReduce 查询分析数据。

Hive是数据仓库工具，**没有集群的概念**，如果想提交Hive作业只需要在hadoop集群 Master节点上装Hive就可以了



##### Hive HQL操作

- 终端输入hive,进入命令行

```mysql
#1-建库建表
CREATE DATABASE test;
SHOW DATABASES;
CREATE TABLE student(classNo string, stuNo string, score int) row format delimited fields terminated by ',';
#加载数据方式1,将数据load到表中(或者用 hadoop fs -put student.txt /user/hive/warehouse/student/)
load data local inpath '/root/tmp/student.txt'overwrite into table student;
#加载数据方式2,用hdfs命令将数据从本地复制至hdfs
hadoop fs -put student.txt /user/hive/warehouse/student003

#查询表中的数据 跟SQL类似
hive>select * from student;
#分组查询group by和统计 count
hive>select classNo,count(score) from student where score>=60 group by classNo;

#----查询结果展示:
OK
C01     2
C02     3
C03     2
Time taken: 130.001 seconds, Fetched: 3 row(s)


#2++++++创建一个外部表student2+++++++
CREATE EXTERNAL TABLE student2 (classNo string, stuNo string, score int) row format delimited fields terminated by ',' location '/tmp/student';
#显示表信息
desc formatted table_name;
#删除表查看结果
drop table student;

#3--------创建分区表
create table employee (name string,salary bigint) partitioned by (date1 string) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;

#4-------查看表的分区-------
show partitions employee;
#5-------添加分区-------
alter table employee add if not exists partition(date1='2018-12-01');
#加载数据到分区
load data local inpath '/root/tmp/employee.txt' into table employee partition(date1='2018-12-01');


```

##### 动态分区

```mysql
#在写入数据时自动创建分区(包括目录结构)
#1创建表
create table employee2 (name string,salary bigint) partitioned by (date1 string) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;
#2导入数据
insert into table employee2 partition(date1) select name,salary,date1 from employee;
#3使用动态分区需要设置参数
set hive.exec.dynamic.partition.mode=nonstrict;
```



##### Hive的内部表和外部表

|                  | 内部表(managed table)                                       | 外部表(external table)                                       |
| ---------------- | ----------------------------------------------------------- | ------------------------------------------------------------ |
| 概念             | 创建表时无external修饰                                      | 创建表时被external修饰                                       |
| 数据管理         | 由Hive自身管理                                              | 由HDFS管理                                                   |
| 数据保存位置     | hive.metastore.warehouse.dir （默认：/user/hive/warehouse） | hdfs中任意位置                                               |
| 删除时影响       | 直接删除元数据（metadata）及存储数据                        | 仅会删除元数据，HDFS上的文件并不会被删除                     |
| 表结构修改时影响 | 修改会将修改直接同步给元数据                                | 表结构和分区进行修改，则需要修复（MSCK REPAIR TABLE table_name;） |



##### 外表用途

- 存在hdfs中的其他路径的文件需要hive管理时,可以使用外表管理,同时不需要移动文件



##### Hive 函数(内,自定义)

- 简单函数: 日期函数 字符串函数 类型转换
- 统计函数: sum avg distinct, uni
- 集合函数
- 分析函数



概念:

- UDF(user-defined function) 用户自定义函数   相当于mapper
- UDAF(User Defined Aggregate Function) 用户自定义聚合函数:  相当于 reducer



使用:

- 可以使用别人已经编译好的.jar文件作为UDF/UDAF
- 也可以使用自己编写的python文件来作为UDF/UDAF
- 先创建python 脚本，要处理的数据都是从sys.stdin 这里输入的
- 把python脚本添加到Hive当中，可以从本地的linux添加，也可以放到HDFS上添加
- 通过Transform调用UDF/UDAF



##### UDF示例

（运行的Java已经编写好的UDF）

- 在hdfs中创建/ user / hive / lib目录

  ```shell
  hadoop fs -mkdir /user/hive/lib
  ```

- 把hive目录下lib / hive-contrib-hive-contrib-1.1.0-cdh5.7.0.jar放到hdfs中

  ```shell
  hadoop fs -put hive-contrib-1.1.0-cdh5.7.0.jar /user/hive/lib/
  ```

- 把集群中的jar包的位置添加到hive中

  ```shell
  hive> add jar hdfs:///user/hive/lib/hive-contrib-1.1.0-cdh5.7.0.jar;
  ```

- 在hive创建中**临时** UDF

```sql
hive> CREATE TEMPORARY FUNCTION row_sequence as 'org.apache.hadoop.hive.contrib.udf.UDFRowSequence'
```

- 在之前的案例中使用**临时**自定义函数（函数功能：添加自增长的行号）

  ```sql
  Select row_sequence(),* from employee;
  ```

- 创建 **非临时**自定义函数

```sql
CREATE FUNCTION row_sequence as 'org.apache.hadoop.hive.contrib.udf.UDFRowSequence' using jar 'hdfs:///user/hive/lib/hive-contrib-1.1.0-cdh5.7.0.jar';
```



##### Python UDF

- 准备案例环境

  - 创建表

    ```sql
    CREATE table u(fname STRING,lname STRING);
    ```

  - 向表中插入数据

    ```sql
    insert into table u2 values('George','washington');
    insert into table u2 values('George','bush');
    insert into table u2 values('Bill','clinton');
    insert into table u2 values('Bill','gates');
    ```

- 编写map风格脚本

  ```python
  import sys
  for line in sys.stdin:
      line = line.strip()
      fname , lname = line.split('\t')
      l_name = lname.upper()
      print '\t'.join([fname, str(l_name)])
  ```

- 通过hdfs向hive中ADD file

  - 加载文件到hdfs

    ```shell
    hadoop fs -put udf.py /user/hive/lib/
    ```

  - hive从hdfs中加载python脚本

    ```shell
    ADD FILE hdfs:///user/hive/lib/udf.py;
    ADD FILE /root/tmp/udf1.py;
    ```

- Transform

```sql
SELECT TRANSFORM(fname, lname) USING 'python udf1.py' AS (fname, l_name) FROM u;
```



##### 案例

- 需求: 根据用户行为以及文章标签筛选出用户最感兴趣(阅读最多)的标签
- 思路:
  - 准备数据
  - 把文章表中关键字列表拆开 lateral view explode
  - 根据文章id找到用户查看文章的关键字, left outer JOIN
  - 根据文章id找到用户查看文章的关键字并统计频率: group by a.user_id,b.kw
  - 将用户查看的关键字和频率合并成 key:value形式 concat_ws(':',b.kw,cast (count(1) as string))
  - 将上面聚合结果转换成map: str_to_map(concat_ws(',',collect_set(cc.kw_w))) 
  - 将用户的阅读偏好结果保存到表中: create table user_kws as 上面的查询语句
  - 从表中通过key查询map中的值:  wm['kw1'] 
  - 从表中获取map中所有的key 和 所有的value: map_keys(wm),map_values(wm) 
  - 用lateral view explode把map中的数据转换成多列:  lateral view explode(wm) t as keyword
- 注意: 
  - Could not connect to meta store using any of the URIs provided 报错信息
  - 拒接连接
  - 说明**元数据**存储没有打开



##### 总结

- 本质
  - 数据仓库的工具
  - 做了两件事儿
    - 元数据的维护
    - 翻译 HQL翻译成 hdfs命令/mapreduce 作业
  - hive优点
    - 不用学mapreduce 就可以处理hdfs上的结构化数据
    - hive比直接写mapreduce代码要少很多，提高开发效率
  - hive问题
    - 只能做离线计算，受到mapreduce速度限制
- **Hive的数据模型** ☆☆☆☆☆
  - 数据库 表 ： hdfs上的文件夹
  - 数据： 表文件夹下的文件
- HQL 常用语句和SQL区别
  - 创建表的时候 需要指定分隔符
  - 加载数据的时候使用 load data 或者直接用hadoop 命令 不建议使用Insert 
- Hive 内部表和外部表 ☆☆☆☆☆
  - 内部表数据是放到指定位置上 /user/hive/warehouse
  - 外部表可以在hdfs上的任意位置
  - 创建外部表的时候需要添加external关键字，并且需要通过location指定数据位置
  - 删除数据的时候 内部表会删除数据和元数据
  - 外部表只删除元数据
- Hive 分区表☆☆☆☆☆
  - 创建表的时候 通过partition by指定分区字段
  - 分区表实际上就是在表目录下创建子文件夹
  - 添加分区的时候通过alter table 表名 partion







#### Hbase列数据库

- HBase是一个分布式的、面向列的开源数据库
- HBase是Google BigTable的开源实现
- HBase不同于一般的关系数据库, 适合非结构化数据存储





##### HBase 与 传统关系数据库的区别

|            | HBase                 | 关系型数据库              |
| ---------- | --------------------- | ------------------------- |
| 数据库大小 | PB级别                | GB ,TB                    |
| 数据类型   | Bytes                 | 丰富的数据类型            |
| 事务支持   | ACID只支持单个Row级别 | 全面的ACID支持, 对Row和表 |
| 索引       | 只支持Row-key         | 支持                      |
| 吞吐量     | 百万写入/秒           | 数千写入/秒               |



##### 关系型数据库的困难

- 简单的事情只要上了量就会变成无比复杂的事情
- Order by 耗性能
- 大量发生,但又无法分布式处理
- 顾客需要实时看到自己的足迹,因此不能使用缓存技巧



##### HBase优势

- 天生就是面向时间戳查询
- 基于行键的查询异常快速,尤其是最近的数据仍存在内存memstore里,完全没有IO开销
- 分布式化解负荷



##### 使用场景

- 成熟的数据分析主题(eg.商品推荐),查询模式已经确立并且不轻易改变
- 传统的关系型数据库已经无法承受负荷,高速插入,大量读取
- 适合海量的,但同时也是简单的操作



##### HBase DDL 和 DML 命令

- 在终端中输入 hbase shell

| 名称                     | 命令表达式                                   |
| ------------------------ | -------------------------------------------- |
| 创建表                   | create '表名', '列族名1','列族名2','列族名n' |
| 添加记录                 | put '表名','行名','列名:','值                |
| 查看记录                 | get '表名','行名'                            |
| 查看表中的记录总数       | count '表名'                                 |
| 删除记录                 | delete '表名', '行名','列名'                 |
| 删除一张表               | 第一步 disable '表名' 第二步 drop '表名'     |
| 查看所有记录             | scan "表名称"                                |
| 查看指定表指定列所有数据 | scan '表名' ,{COLUMNS=>'列族名:列名'}        |
| 更新记录                 | 重写覆盖                                     |

类似Redis,key-value





##### HappyBase操作HBase

- 目标: 能够创建连接,关闭链接, 创建表, 删除表, 查询表

- 把HBase 的 thrift server启动: hbase-daemon.sh start thrift

  - To start the Thrift server run

     'hbase-daemon.sh start thrift' or 'hbase thrift'

- 先获取 Connection

- 通过connection对象可以对表进行操作

  - 创建表 connection.create_table('user2',{'cf1':dict()})
  - 查询表 connection.tables()
  - 删除表 connection.delete_table('user2',True)

- 通过Connection获取table

  - 通过table对数据进行操作

- 查询

  - scan
  - get

```python
#连接集群
hbase shell
#创建表
create 'user','base_info'
#删除表
disable 'user'
drop 'user'
#创建名称空间
create_namespace 'test'
#展示现有名称空间
list_namespace
#创建表的时候添加namespace
create 'test:user','base_info'
#显示某个名称空间下有哪些表
list_namespace_tables 'test'
#插入数据
put ‘表名’，‘rowkey的值’，’列族：列标识符‘，’值‘
put 'user','rowkey_10','base_info:username','Tom'
```


```python
#!/c/Users/struggle6/AppData/Local/Programs/Python/Python37/python

import happybase
def getQuery():
    connection = happybase.Connection('192.168.19.137')
    # 通过connection找到user表 获得table对象
    table = connection.table('user')
    result = table.row('rowkey_22',columns=['base_info:username'])
    #result = table.row('rowkey_22',columns=['base_info:username'])
    result = table.rows(['rowkey_22','rowkey_16'],columns=['base_info:username'])
    print(result)
    # 关闭连接
    connection.close()

getQuery()
```



##### HBase组件☆☆☆☆

- ZK(Zookeeper)
  - 服务的注册与发现，HMaster和HRegionServer都向zk注册地址，客户端向zk请求HMaster和HRegionServer的地址
  - HMaster主节点选举
  - HMaster元数据存储
  - HRegionServer状态汇报
- HMaster
  - 表数据的CRUD(增删改查)
  - HRegionServer Region划分
  - Region负载均衡、监控Region状态
- HRegionServer
  - 数据存储
  - 切分在运行过程中变得过大的region
- HRegionServer->HRegion->HStore（对应一个列族） Memstore，storefile

- Write-Ahead-Log（WAL）保障数据高可用
  - 在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中（HLog文件格式见后续），HLog文件定期会滚动出新的，并删除旧的文件（已持久化到StoreFile中的数据）



##### HBase物理存储原理

- Flush
  - 当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile）
- Compact
  - Minor compact
    - 小的相邻的StoreFile合并成一个大的StoreFile，不会处理数据，只是合起来
  - Major compact
    - 除了文件合并，还有很多其它任务，删除过期数据
- Split 
  - 当当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前 Region Split成2个



##### HBase自动split带来的问题

- 自动split带来的问题
  - 当当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前 Region Split成2个
  - 父Region会下线, 会出来两个新的Region
  - 旧的任务下线导致mr job 崩溃
- 如何解决: 
  - 尽量避免自动Region Split (hbase-site.xml设置参数)







### Spark大数据计算平台



       ____              __
      / __/__  ___ _____/ /__
     _\ \/ _ \/ _ `/ __/  '_/
    /__ / .__/\_,_/_/ /_/\_\   version 2.2.2
          /_/




目前，Spark已经发展成为包含众多子项目的大数据计算平台。 伯克利将Spark的整个生态系统称为伯克利数据分析栈（BDAS）。 其核心框架是Spark，同时BDAS涵盖支持结构化数据SQL查询与分析的查询引擎Spark SQL和Shark，提供机器学习功能的系统MLbase及底层的分布式机器学习库MLlib、 并行图计算框架GraphX、 流计算框架Spark Streaming、 采样近似计算查询引擎BlinkDB、 内存分布式文件系统Tachyon、 资源管理框架Mesos等子项目。 这些子项目在Spark上层提供了更高层、 更丰富的计算范式。

#### Spark生态

![img](https://images2015.cnblogs.com/blog/855959/201607/855959-20160726115216528-1598079432.png)





#### Spark特点

- speed
- ease of use
- generality
- runs everywhere



#### Spark框架组件

- 批处理: Spark core,Spark SQL
- 交互式计算 Spark SQL
- 流式计算:Spark Streaming
- 机器学习:Spark ML,Spark MLLib

缺点

- 内存消耗大



#### pyspark

```python
sc = spark.sparkContext
words = sc.textFile('file:///tmp/test_0.txt').flatMap(lambda line: line.split(" ")) .map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b).collect()
```





#### RDD概述

RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合.

- spark 最基础的数据结构
- 弹性分布式数据集
  - 弹性
  - 不可变
    - RDD1 变换到RDD2 
  - 可分区
    - 创建RDD的时候可以指定partition的数量
    - partition的数量可以根据申请的CPU内核数决定
    - 一个partition对应一个task,一个task对应一个线程
    - 1一个内核对应2~4个task对应2~4个partition
- 创建RDD
  - spark context
  - sc





```
Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)
SparkSession available as 'spark'.
>>> sc
<SparkContext master=local[*] appName=PySparkShell>
>>> data =[1,2,3,4,5]
>>> rdd3 =sc.parallelize(data)
>>> rdd3
ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:498
>>> rdd3.collect()
[1, 2, 3, 4, 5]                                                                 

>>> rdd1 =sc.textFile('file:///tmp/test_0.txt')
>>> rdd1
file:///tmp/test_0.txt MapPartitionsRDD[8] at textFile at NativeMethodAccessorImpl.java:0>>> rdd1.collect()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/root/bigdata/spark/python/pyspark/rdd.py", line 816, in collect
    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "/root/bigdata/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__

```



#### RDD的三类算子

- transformation
  - 处理rdd之间变换
  - 延迟执行，不调用action，不会得到结果。分析，优化
  - union
  - map
  - filter
  - flatmap
  - intersection
  - groupBykey
  - reduceBykey
  - sortBykey
- action
  - 获取计算结果，内容，rdd1.collect()
  - count
  - parallelize
  - reduce
  - first
  - take
- persist
  - 负责数据存储的算子



#### IP地理位置统计

- mapPartitions
  - 数据分成多份
  - 如果在数据处理中，需要连接其他资源，做耗时操作，推荐使用
- 广播变量





#### standalone

- 启动

  - ```shell
    ./start-master.sh -h 192.168.19.137
    ./start-slave.sh spark://192.168.19.137:7077
    ```

- 集群角色

  - Application
  - Master 集群的管理者
    - 接受作业 下发给slave
    - 监控 slave节点状态
    - slave节点如果出错调度任务到其它节点
  - Worker 负责具体计算的节点
    - 监控自身的健康
    - 响应Master下发的作业
    - 启动 Driver和Executor执行作业
  - Driver Executor

- 作业相关概念

  - stage 作业会被划分成1~n个阶段
    - 窄依赖
      - 父RDD的一个partition只指向子RDD的一个partition
    - 宽依赖
      - 子RDD的每一个partition都依赖于父RDD的所有partition
    - stage划分依据
      - 从RDD1变换到RDD2如果是窄依赖属于同一个stage
      - 只要遇到了宽依赖，当前的stage结束
  - DAGScheduler：stage的划分由DAGScheduler 完成的，每个Stage根据RDD的Partition个数决定Task的个数
  - TaskScheduler 把具体的task调度到Executor上执行







#### Spark与Hadoop生态系统对比

#### Spark与Hadoop对比

MapReduce与Spark对比

Spark与Hadoop协作





#### Spark SQL

shark(Hive on Spark,Spark SQL)

![1566179092535](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566179092535.png)

- spark sql 作用
  - **处理的是结构化的数据**
  - SQL 翻译成 RDD
  - DataFrameAPI  翻译成 RDD
- spark sql 优势
  - SQL/DataFrame 代码更少
  - SQL/DataFrame 运行速度快
- Spark SQL的特性
  - 跟Spark Core 无缝兼容
  - 连接各种数据源使用统一的方式
  - 跟Hive兼容很好，可以使用HQL，可以读取hive的元数据信息
  - 可以把spark sql处理好的数据通过标准的接口（JDBC or ODBC） 暴露出去



#### Spark SQL 的DataFrame

- RDD 的特性 DataFrame都有
  - 不可变
  - tranformation 延迟执行
  - 分布式

- DataFrame是带着schema的RDD，比RDD的API更丰富，执行效率比直接写RDD更高
- 对比pandas 的 DataFrame
  - 可以处理分布式的数据，处理的数据量更大
  - api没有pandas DataFrame丰富





#### DataFrame的创建

- 先要有一个SparkSession

  ```python
  import os
  # 配置spark driver和pyspark运行时，所使用的python解释器路径
  PYSPARK_PYTHON = "/miniconda2/envs/py365/bin/python"
  JAVA_HOME='/root/bigdata/jdk1.8.0_191'
  # 当存在多个版本时，不指定很可能会导致出错
  os.environ["PYSPARK_PYTHON"] = PYSPARK_PYTHON
  os.environ["PYSPARK_DRIVER_PYTHON"] = PYSPARK_PYTHON
  os.environ['JAVA_HOME']=JAVA_HOME
  # spark配置信息
  from pyspark import SparkConf
  from pyspark.sql import SparkSession
  
  SPARK_APP_NAME = "SparkSQLTest"
  SPARK_URL = "spark://192.168.2.137:7077"
  
  conf = SparkConf()    # 创建spark config对象
  config = (
  	("spark.app.name", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称
  	("spark.executor.memory", "6g"),    # 设置该app启动时占用的内存用量，默认1g
  	("spark.master", SPARK_URL),    # spark master的地址
      ("spark.executor.cores", "4"),    # 设置spark executor使用的CPU核心数
  )
  # 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html
  
  conf.setAll(config)
  
  # 利用config对象，创建spark session
  spark = SparkSession.builder.config(conf=conf).getOrCreate()
  ```

- 通过SparkSession可以创建DataFrame

  - sparkSession.createDataFrame
  - sparkSession.read.XXX

- DataFrame的api

  - DataFrame之间变换的操作 transformation
  - 获取DataFrame数据的 action
  - 其它基本操作

#### DataFrame操作CSV和Json数据

- 加载csv/json
  - spark.read.csv(文件路径)
  - spark.read.json(文件路径)
  - spark.read.format('文件的格式').load(文件路径)
- 加载文件之后 会自动推断文件的schema
- 也可以自己创建schema做修改
  - StructType()
  - 可以创建StructType对象之后add每一个字段
  - 也可以在创建StructType的时候，直接传入一个StructField("id", StringType（））的list
- spark.read.schema(jsonSchema).json()
- spark SQL udf的套路
  - 创建一个方法
  - 调用udf函数传入 方法名字 参数类型 创建一个udf对象
  - 调用这个udf对象 ，而不是直接使用自己创建的方法

- DataFrame数据.rdd 可以转换成RDD去操作，经过RDD的算子处理之后，可以在通过toDF转换回DataFrame



#### 重点

RDD的transformation 和 action算子的使用

DataFrame 和 RDD之间的关系

DataFrame具体的使用



#### SQL on Hadoop

- Hive
- impala
- presto
- drill
- Spark SQL



启动centos的jupyter

source activate py365 进入虚拟环境（centos，未安装workon功能）

deactivate 退出

jupyter notebook --ip 0.0.0.0 --allow-root



#### Spark ML

<http://spark.apache.org/docs/latest/ml-guide.html>

- LinearRegression

```python
from pyspark.ml.regression import LinearRegression

# Load training data
training = spark.read.format("libsvm")\
    .load("data/mllib/sample_linear_regression_data.txt")

lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)

# Fit the model
lrModel = lr.fit(training)

# Print the coefficients and intercept for linear regression
print("Coefficients: %s" % str(lrModel.coefficients))
print("Intercept: %s" % str(lrModel.intercept))

# Summarize the model over the training set and print out some metrics
trainingSummary = lrModel.summary
print("numIterations: %d" % trainingSummary.totalIterations)
print("objectiveHistory: %s" % str(trainingSummary.objectiveHistory))
trainingSummary.residuals.show()
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)
```



- RandomForestRegressor

```python
from pyspark.ml import Pipeline
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.feature import VectorIndexer
from pyspark.ml.evaluation import RegressionEvaluator #评估

# Load and parse the data file, converting it to a DataFrame.
data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

# Automatically identify categorical features, and index them.
# Set maxCategories so features with > 4 distinct values are treated as continuous.
featureIndexer =\
    VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(data)

# Split the data into training and test sets (30% held out for testing)
(trainingData, testData) = data.randomSplit([0.7, 0.3])

# Train a RandomForest model.
rf = RandomForestRegressor(featuresCol="indexedFeatures")

# Chain indexer and forest in a Pipeline
pipeline = Pipeline(stages=[featureIndexer, rf])

# Train model.  This also runs the indexer.
model = pipeline.fit(trainingData)

# Make predictions.
predictions = model.transform(testData)

# Select example rows to display.
predictions.select("prediction", "label", "features").show(5)

# Select (prediction, true label) and compute test error
evaluator = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)

rfModel = model.stages[1]
print(rfModel)  # summary only
```





#### Spark数据清洗

- 去重

```python
# 查看重复记录
#无意义重复数据去重：数据中行与行完全重复
# 1.首先删除完全一样的记录
df2 = df.dropDuplicates()

#有意义去重：删除除去无意义字段之外的完全重复的行数据
# 2.其次，关键字段值完全一模一样的记录（在这个例子中，是指除了id之外的列一模一样）
# 删除某些字段值完全一样的重复记录，subset参数定义这些字段
df3 = df2.dropDuplicates(subset = [c for c in df2.columns if c!='id'])
# 3.有意义的重复记录去重之后，再看某个无意义字段的值是否有重复（在这个例子中，是看id是否重复）
# 查看某一列是否有重复值
import pyspark.sql.functions as fn
df3.agg(fn.count('id').alias('id_count'),fn.countDistinct('id').alias('distinct_id_count')).collect()
# 4.对于id这种无意义的列重复，添加另外一列自增id

df3.withColumn('new_id',fn.monotonically_increasing_id()).show()
```



- 缺失值处理

toPandas()



- 异常值处理

![1566612252610](C:\Users\struggle6\AppData\Roaming\Typora\typora-user-images\1566612252610.png)





#### SparkStreaming概述

- 它是一个可扩展，高吞吐具有容错性的流式计算框架

  吞吐量：单位时间内成功传输数据的数量

之前我们接触的spark-core和spark-sql都是处理属于离线批处理任务，数据一般都是在固定位置上，通常我们写好一个脚本，每天定时去处理数据，计算，保存数据结果。这类任务通常是T+1(一天一个任务)，对实时性要求不高。

- 特点
  - 准实时的分布式计算框架，延迟比较低，间隔1s左右
  - 采用micro-batch处理数据，Min-batch小批量
  - 指定一个时间间隔，每隔相同的一段时间就到数据源取数据 
  - 只能stop一次，默认停止SparkStreamingj时会同时关闭SparkContext
  - 对比storm生态更丰富，python支持更友好



##### 实时计算框架对比

Storm

- 流式计算框架
- 以record为单位处理数据
- 也支持micro-batch方式（Trident）

Spark

- 批处理计算框架
- 以RDD为单位处理数据
- 支持micro-batch流式处理数据（Spark Streaming）

对比：

- 吞吐量：Spark Streaming优于Storm
- 延迟：Spark Streaming差于Storm



##### SparkStreaming组件

- Streaming Context
  - 流式计算的上下文
  - 通过spark context创建streaming context
- DStream 离散流
  - SparkStreaming数据抽象
  - 由一系列的RDD组成
- 关于Streaming Context几点说明





#### SparkStreaming的有状态操作

- updateStateByKey
  - 会保留之前结果





#### SparkStreaming编码步骤

- 创建一个StreamingContext
- 从StreamingContext中创建一个数据对象
- 对数据对象进行Transformations操作
- 输出结果
- 开始和停止











#### Spark分布式计算框架

##### Spark core

- spark context
- RDD
  - transformation
  - action
  - persist

##### Spark SQL

- spark session
- DataFrame

##### Spark Streaming

- streaming context
- DStream



##### Hive数据仓库

- HQL翻译成MapReduce或者hdfs命令
- 使用场景（数据分析，大数据相关，数仓etl）在推荐业务中更多还是提供一个元数据服务
- 结构
  - 用户接口，命令行
  - 元数据存储mysql
  - 驱动
- 数据模型
  - 数据库表，分区表：文件夹
  - 数据：具体文件
- 数据类型：hive支持复杂数据类型array map struct
- 内部表，外部表，分区表
- UDF,UDAF自定义函数，实际是在写MapReduce
- explode ,lateral view explode
- collect_set,collect_list contract,contract_ws,str_to_map





##### HBase分布式列数据库

- happybase操作HBase
  - 创建connection,建立连接
    - 可以对表做管理
  - table
    - 通过table可以对数据进行管理
  - 查询
    - scan
      - 全表搜索
      - startrow,endrow指定rowkey范围endrow
    - get
      - 先命中rowkey
- HBase的数据模型
  - 不区分数据类型，byte
  - namespace
  - 创建表的时候，只需要指定表名，列族名字
  - 行row_key
  - 列族 cf:cq:value
  - 列标识符
  - 时间戳，版本号
- HBase集群角色
  - HMaster
  - HRegionServer
    - HRegion
    - Hstore
      - storefile
      - memstore
  - Zookeeper
- flush compact split操作
  - 避免HBase自动compact split





#### 推荐算法回顾

- 信息过滤系统
- 信息过载，用户需求不明确
- 流程：召回，过滤调整，排序，过滤调整
- 召回recall
  - 协同过滤CF
    - user item matrix
    - 用户对物品的评分矩阵
    - 如果用户对物品的评分数据**稠密**
      - 基于用户的协同过滤
        - 通过用户的消费记录，找相似用户
        - 构建用户向量，计算微量之间相似度
      - 基于物品的协同过滤
        - 通过用户消费记录，找相似物品
        - 构建物品向量，计算微量之间相似度
    - 如果用户对物品的评分数据**稀疏**
      - Surprise库
      - ModelBasedCF
        - BaseLine
        - 矩阵分解LFM
          - 大矩阵拆分成两个小矩阵
          - 隐因子的个子要远远小于矩阵的维度
  - 基于内容CB
    - 物品画像
      - TF-IDF
      - 建立关键词对物品索引
    - 如果有用户行为数据
      - 建立用户消费过的物品对的关键词建立用户画像
      - 用户画像的标签对物品索引
      - 利用用户消费的次数，用户的评分，进行降序
    - 没有用户行为数据
      - 标签->词向量
      - 物品（多个标签）->文档向量
      - 计算微量的相似度找相似物品
- 排序ranking
  - LR
    - 用户的特征 + 物品的特征，预测用户是否会点击





btag:行为类型（单个特征设参数上限，防止异常值）

- pv浏览
- cart加入购物车
- fav喜欢
- buy购买



#### 电商推荐案例数据介绍

- 目标: 知道电商项目都用到哪些数据集
- 原始样本骨架raw_sample
  - user_id：脱敏过的用户ID；
  - adgroup_id：脱敏过的广告单元ID；
  - time_stamp：时间戳；
  - pid：资源位；
  - noclk：为1代表没有点击；为0代表点击；
  - clk：为0代表没有点击；为1代表点击；
- 广告基本信息表ad_feature
  - adgroup_id：脱敏过的广告ID；
  - cate_id：脱敏过的商品类目ID；
  - campaign_id：脱敏过的广告计划ID；
  - customer_id: 脱敏过的广告主ID；
  - brand_id：脱敏过的品牌ID；
  - price: 宝贝的价格
- 用户基本信息表user_profile
  - userid：脱敏过的用户ID；
  - cms_segid：微群ID；
  - cms_group_id：cms_group_id；
  - final_gender_code：性别 1:男,2:女；
  - age_level：年龄层次； 1234
  - pvalue_level：消费档次，1:低档，2:中档，3:高档；
  - shopping_level：购物深度，1:浅层用户,2:中度用户,3:深度用户
  - occupation：是否大学生 ，1:是,0:否
  - new_user_class_level：城市层级
- 用户的行为日志behavior_log
  - user：脱敏过的用户ID；
  - time_stamp：时间戳； 
  - btag：行为类型, 包括以下四种类型 
    - pv  浏览 ​
    - cart 加入购物车 ​ 
    - fav  喜欢 
    - buy  购买
  - cate_id 脱敏过的商品类目id； 
  - brand_id: 脱敏过的品牌id；
- 数据集的下载地址: https://tianchi.aliyun.com/dataset/dataDetail?dataId=56







#### 模型构建思路介绍:v::helicopter::laughing:

- 理解模型构建的思路
- 推荐业务处理主要流程： 召回 ===> 排序 ===> 过滤
  - 离线处理业务流
    - raw_sample.csv + ad_feature.csv + user_profile.csv ==> CTR点击率预测模型
    - behavior_log.csv ==> 评分数据 ==> user-cate/brand评分数据 ==> 协同过滤 ==> top-N cate/brand ==> 关联广告
    - 协同过滤召回 ==> top-N cate/brand ==> 关联对应的广告完成召回
  - 在线处理业务流
    - 数据处理部分：
      - 实时行为日志 ==> 实时特征 ==> 缓存
      - 实时行为日志 ==> 实时商品类别/品牌 ==> 实时广告召回集 ==> 缓存
    - 推荐任务部分：
      - CTR点击率预测模型 + 广告/用户特征(缓存) + 对应的召回集(缓存) ==> 点击率排序 ==> top-N 广告推荐结果
- 涉及技术：
  - Flume：日志数据收集
  - Kafka：实时日志数据处理队列
  - HDFS：存储数据
  - Spark SQL：离线处理
  - Spark ML：模型训练
  - Redis：缓存



##### CTR预估

- 计算用户点击广告（产品）的概率
- 在推荐中传入用户的ID，根据用户ID找到召回集，根据用户的特征和召回集的物品牲计算每一件商品的点击率
- 根据点击率降序排序 



- 在训练模型的时候（数据处理）处理缺失值
  - 随机森林预测缺失值
  - one-hot在spark中如何进行onehot的处理

- 训练召回模型
- 训练排序模型
  - spark中如何训练逻辑回归模型
- - 



- 数据缓存
  - 数据换存至redis
- 实时推荐



- StringIndexer
  - 把分类数据，转换成0开始的分类值
- OneHotEncoder
  - 在StringIndexer基础上，把数据处理成OneHot的形式
  - 输出的OneHot的结果，使用SparseVector（稀疏向量)的形式来表示
  - 创建OneHotEncoder对象的时候有一个参数droplast一般传入False，如果传入True那么会丢掉OneHot最后一位，即三分类的数据使用二维向量来表示
  - OneHotEncoder.transform(dataframe)





















